"""
sess_load_util.py

This module contains functions for loading data from files generated by the 
Allen Institute OpenScope experiments for the Credit Assignment Project.

Authors: Colleen Gillon

Date: August, 2018

Note: this code uses python 3.7.

"""

import copy
from pathlib import Path

import cv2
import h5py
import json
import numpy as np
import pandas as pd
import pynwb

from util import file_util, gen_util, logger_util
from sess_util import sess_file_util, sess_gen_util, sess_sync_util

TAB = "    "

NWB_FILTER_KS = 5


logger = logger_util.get_module_logger(name=__name__)


######################################
def get_sessid_from_mouse_df(mouse_n=1, sess_n=1, runtype="prod", 
                             mouse_df="mouse_df.csv"):
    """
    get_sessid_from_mouse_df(sessid)

    Returns session ID, based on the mouse number, session number, and runtype,
    based on the mouse dataframe.

    Optional args:
        - mouse_n (int)  : mouse number
                           default: 1
        - sess_n (int)   : session number
                           default: 1
        - runtype (str)  : type of data
                           default: 1
        - mouse_df (Path): path name of dataframe containing information on each 
                           session. Dataframe should have the following columns:
                               mouse_n, sess_n, runtype
                           default: "mouse_df.csv"

    Returns:
        - sessid (int): session ID
    """

    if isinstance(mouse_df, (str, Path)):
        mouse_df = file_util.loadfile(mouse_df)

    df_line = gen_util.get_df_vals(
        mouse_df, ["mouse_n", "sess_n", "runtype"], 
        [int(mouse_n), int(sess_n), runtype],
        single=True
        )

    sessid = int(df_line["sessid"].tolist()[0])

    return sessid


######################################
def load_info_from_mouse_df(sessid, mouse_df="mouse_df.csv"):
    """
    load_info_from_mouse_df(sessid)

    Returns dictionary containing information from the mouse dataframe.

    Required args:
        - sessid (int): session ID

    Optional args:
        - mouse_df (Path): path name of dataframe containing information on each 
                           session. Dataframe should have the following columns:
                               sessid, mouse_n, depth, plane, line, sess_n, 
                               pass_fail, all_files, any_files, notes
                           default: "mouse_df.csv"

    Returns:
        - df_dict (dict): dictionary with following keys:
            - age_weeks (float): age (in weeks)
            - all_files (bool) : if True, all files have been acquired for
                                 the session
            - any_files (bool) : if True, some files have been acquired for
                                 the session
            - timestamp (str)  : session timestamp (in UTC)
            - date (str)       : session date (i.e., yyyymmdd)
            - depth (int)      : recording depth 
            - DOB (int)        : date of birth (i.e., yyyymmdd)
            - plane (str)      : recording plane ("soma" or "dend")
            - line (str)       : mouse line (e.g., "L5-Rbp4")
            - mouse_n (int)    : mouse number (e.g., 1)
            - mouseid (int)    : mouse ID (6 digits)
            - notes (str)      : notes from the dataframe on the session
            - pass_fail (str)  : whether session passed "P" or failed "F" 
                                 quality control
            - runtype (str)    : "prod" (production) or "pilot" data
            - sess_n (int)     : overall session number (e.g., 1)
            - sex (str)        : sex (e.g., "F" or "M")
            - stim_seed (int)  : random seed used to generated stimulus 
    """

    if isinstance(mouse_df, (str, Path)):
        mouse_df = file_util.loadfile(mouse_df)

    df_line = gen_util.get_df_vals(mouse_df, "sessid", sessid, single=True)

    df_dict = {
        "mouse_n"      : int(df_line["mouse_n"].tolist()[0]),
        "timestamp"    : df_line["full_timestamp"].tolist()[0],
        "sex"          : str(df_line["sex"].tolist()[0]),
        "DOB"          : int(df_line["DOB"].tolist()[0]),
        "date"         : int(df_line["date"].tolist()[0]),
        "age_weeks"    : float(df_line["age_weeks"].tolist()[0]),
        "depth"        : df_line["depth"].tolist()[0],
        "plane"        : df_line["plane"].tolist()[0],
        "line"         : df_line["line"].tolist()[0],
        "mouseid"      : int(df_line["mouseid"].tolist()[0]),
        "runtype"      : df_line["runtype"].tolist()[0],
        "sess_n"       : int(df_line["sess_n"].tolist()[0]),
        "stim_seed"    : int(df_line["stim_seed"].tolist()[0]),
        "pass_fail"    : df_line["pass_fail"].tolist()[0],
        "all_files"    : bool(int(df_line["all_files"].tolist()[0])),
        "any_files"    : bool(int(df_line["any_files"].tolist()[0])),
        "notes"        : df_line["notes"].tolist()[0],
    }

    return df_dict


#############################################
def get_mouseid_sessid_nwb(nwb_files):
    """
    get_mouseid_sessid_nwb(nwb_files)

    Returns the mouse ID and session ID retrieve from NWB files. If several 
    files are passed, they are expected to be for the same session 
    (with identical mouse ID and session IDs).

    Required args:
        - nwb_files (Path or list): path(s) to the NWB file(s) for a single 
                                    session
    
    Returns:
        - mouseid (str): mouse ID (Allen)
        - sessid (str) : session ID (Allen)
    """

    nwb_files = gen_util.list_if_not(nwb_files)

    mouseid, sessid = None, None
    for nwb_file in nwb_files:
        with pynwb.NWBHDF5IO(str(nwb_file), "r") as f:
            nwbfile_in = f.read()  
            new_mouseid = nwbfile_in.subject.subject_id
            if mouseid is None:
                mouseid = new_mouseid
            elif mouseid != new_mouseid:
                nwb_filenames = [str(filename) for filename in nwb_files]
                raise RuntimeError(
                    "Mouse IDs for different NWB files for the same session "
                    f"do not match: {', '.join(nwb_filenames)}."
                    )
            
            new_sessid = nwbfile_in.session_id
            if sessid is None:
                sessid = new_sessid
            elif sessid != new_sessid:
                nwb_filenames = [str(filename) for filename in nwb_files]
                raise RuntimeError(
                    "Session IDs for different NWB files for the same session "
                    f"do not match: {', '.join(nwb_filenames)}."
                    )

    return mouseid, sessid


#############################################
def load_small_stim_pkl(stim_pkl, runtype="prod"):
    """
    load_small_stim_pkl(stim_pkl)

    Loads a smaller stimulus dictionary from the stimulus pickle file in which 
    "posbyframe" for visual flow stimuli is not included. 
    
    If it does not exist, small stimulus dictionary is created and saved as a
    pickle with "_small" appended to name.
    
    Reduces the pickle size about 10 fold.

    Required args:
        - stim_pkl (Path): full path name for the full stimulus pickle file
    
    Optional args:
        - runtype (str): runtype ("prod" or "pilot")
    """

    stim_pkl = Path(stim_pkl)
    stim_pkl_no_ext = Path(stim_pkl.parent, stim_pkl.stem)
    small_stim_pkl_name = Path(f"{stim_pkl_no_ext}_small.pkl")
    
    if small_stim_pkl_name.is_file():
        return file_util.loadfile(small_stim_pkl_name)
    else:
        logger.info("Creating smaller stimulus pickle.", extra={"spacing": TAB})

        stim_dict = file_util.loadfile(stim_pkl)

        if runtype == "pilot":
            stim_par_key = "stimParams"
        elif runtype == "prod":
            stim_par_key = "stim_params"
        else:
            gen_util.accepted_values_error(
                "runtype", runtype, ["prod", "pilot"])

        for i in range(len(stim_dict["stimuli"])):
            stim_keys = stim_dict["stimuli"][i][stim_par_key].keys()
            stim_par = stim_dict["stimuli"][i][stim_par_key]
            if runtype == "pilot" and "posByFrame" in stim_keys:
                _ = stim_par.pop("posByFrame")
            elif runtype == "prod" and "square_params" in stim_keys:
                _ = stim_par["session_params"].pop("posbyframe")
                
        file_util.saveinfo(stim_dict, small_stim_pkl_name)

        return stim_dict


#############################################
def load_stim_df_info(stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid, 
                      runtype="prod"):
    """
    load_stim_df_info(stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid)

    Creates the alignment dataframe (stim_df) and saves it as a pickle
    in the session directory, if it does not already exist. Returns dataframe, 
    alignment arrays, and frame rate.
    
    Required args:
        - stim_pkl (Path)    : full path name of the experiment stim pickle 
                               file
        - stim_sync_h5 (Path): full path name of the experiment sync hdf5 file
        - time_sync_h5 (Path): full path name of the time synchronization hdf5 
                               file
        - align_pkl (Path)   : full path name of the output pickle file to 
                               create
        - sessid (int)       : session ID, needed the check whether this 
                               session needs to be treated differently 
                               (e.g., for alignment bugs)

    Optional args:
        - runtype (str): runtype ("prod" or "pilot")
                         default: "prod"

    Returns:
        - stim_df (pd DataFrame): stimlus alignment dataframe with columns:
                                    "stimtype", "unexp", "stim_seg", "gabfr", 
                                    "gab_ori", "gabk", "visflow_dir", 
                                    "visflow_size", "start_twop_fr", 
                                    "end_twop_fr", "num_twop_fr"
        - stimtype_order (list) : stimulus type order
        - stim2twopfr (1D array): 2p frame numbers for each stimulus frame, 
                                  as well as the flanking
                                  blank screen frames 
        - twop_fps (num)        : mean 2p frames per second
        - twop_fr_stim (int)    : number of 2p frames recorded while stim
                                  was playing
    """

    align_pkl = Path(align_pkl)
    sessdir = align_pkl.parent

    # create stim_df if doesn't exist
    if not align_pkl.is_file():
        logger.info(f"Stimulus alignment pickle not found in {sessdir}, and "
            "will be created.", extra={"spacing": TAB})
        sess_sync_util.get_stim_frames(
            stim_pkl, stim_sync_h5, time_sync_h5, align_pkl, sessid, runtype, 
            )
        
    align = file_util.loadfile(align_pkl)

    stim_df = align["stim_df"]
    stim_df = stim_df.rename(
        columns={"GABORFRAME": "gabfr", 
                 "surp": "unexp", # rename surprise to unexpected
                 "stimType": "stimtype",
                 "stimSeg": "stim_seg",
                 "start_frame": "start_twop_fr", 
                 "end_frame": "end_twop_fr", 
                 "num_frames": "num_twop_fr"})
    
    # rename bricks -> visflow
    stim_df["stimtype"] = stim_df["stimtype"].replace({"b": "v"})

    stim_df = modify_visflow_segs(stim_df, runtype)
    stim_df = stim_df.sort_values("start_twop_fr").reset_index(drop=True)

    # note: STIMULI ARE NOT ORDERED IN THE PICKLE
    stimtype_map = {
        "g": "gabors", 
        "v": "visflow"
        }
    stimtype_order = stim_df["stimtype"].map(stimtype_map).unique()
    stimtype_order = list(
        filter(lambda s: s in stimtype_map.values(), stimtype_order))

    # split stimPar1 and stimPar2 into all stimulus parameters
    stim_df["gab_ori"] = stim_df["stimPar1"]
    stim_df["gabk"] = stim_df["stimPar2"]
    stim_df["visflow_size"] = stim_df["stimPar1"]
    stim_df["visflow_dir"] = stim_df["stimPar2"]

    stim_df = stim_df.drop(columns=["stimPar1", "stimPar2"])

    for col in stim_df.columns:
        if "gab" in col:
            stim_df.loc[stim_df["stimtype"] != "g", col] = -1
        if "visflow" in col:
            stim_df.loc[stim_df["stimtype"] != "v", col] = -1

    # expand on direction info
    for direc in ["right", "left"]:
        stim_df.loc[(stim_df["visflow_dir"] == direc), "visflow_dir"] = \
            sess_gen_util.get_visflow_screen_mouse_direc(direc)

    stim2twopfr  = align["stim_align"].astype("int")
    twop_fps     = sess_sync_util.get_frame_rate(stim_sync_h5)[0] 
    twop_fr_stim = int(max(align["stim_align"]))

    return stim_df, stimtype_order, stim2twopfr, twop_fps, twop_fr_stim


#############################################
def load_max_projection_nwb(sess_files):
    """
    load_max_projection_nwb(sess_files)

    Returns maximum projection image of downsampled z-stack as an array, from 
    NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Returns:
        - max_proj (2D array): maximum projection image across downsampled 
                               z-stack (hei x wei), with pixel intensity 
                               in 0 (incl) to 256 (excl) range 
                               ("uint8" datatype).
    """

    ophys_file = sess_file_util.select_nwb_sess_path(sess_files, ophys=True)

    with pynwb.NWBHDF5IO(str(ophys_file), "r") as f:
        nwbfile_in = f.read()
        ophys_module = nwbfile_in.get_processing_module("ophys")
        main_field = "PlaneImages"
        data_field = "max_projection"
        try:
            max_proj = ophys_module.get_data_interface(
                main_field).get_image(data_field)[()].astype("uint8")
        except KeyError as err:
            raise KeyError(
                "Could not find a maximum projection plane image "
                f"for {ophys_file} due to: {err}"
                )

    return max_proj


#############################################
def load_max_projection(max_proj_png):
    """
    load_max_projection(max_proj_png)

    Returns maximum projection image of downsampled z-stack as an array. 

    Required args:
        - max_proj_png (Path): full path names of the maximum projection png

    Returns:
        - max_proj (2D array): maximum projection image across downsampled 
                               z-stack (hei x wei), with pixel intensity 
                               in 0 (incl) to 256 (excl) range 
                               ("uint8" datatype).
    """

    if not Path(max_proj_png).is_file():
        raise OSError(f"{max_proj_png} does not exist.")

    import imageio
    max_proj = imageio.imread(max_proj_png).astype("uint8")

    return max_proj


#############################################
def get_registration_transform_params(nway_match_path, sessid, targ_sess_idx=0):
    """
    get_registration_transform_params(nway_match_path, sessid)

    Returns cv2.warpPerspective registration transform parameters used to 
    register session planes to one another, saved in the n-way match files. 

    (cv2.warpPerspective should be used with flags cv2.INTER_LINEAR and 
    cv2.WARP_INVERSE_MAP)

    Required args:
        - nway_match_path (Path): full path name of the n-way registration path 
                                  (should be a local path in a directory that 
                                  other session registrations are also stored)
        - sessid (int)          : session ID

    Optional args:
        - targ_sess_idx (int): session that the registration transform should 
                               be targetted to
                               default: 0  

    Returns:
        - transform_params (3D array): registration transformation parameters 
                                       for the session (None if the session was 
                                       the registration target)
    """

    if not Path(nway_match_path).is_file():
        raise OSError(f"{nway_match_path} does not exist.")

    with open(nway_match_path, "r") as f:
        nway_metadata = pd.DataFrame().from_dict(json.load(f)["metadata"])

    if len(nway_metadata) != 1:
        raise NotImplementedError(
            "Metadata dataframe expected to only have one line."
            )
    nway_row = nway_metadata.loc[nway_metadata.index[0]]
    if sessid not in nway_row["sess_ids"]:
        raise RuntimeError(
            "sessid not found in the n-way match metadata dataframe."
            )
    sess_idx = nway_row["sess_ids"].index(sessid)
    sess_n = nway_row["sess_ns"][sess_idx]
    n_reg_sess = len(nway_row["sess_ids"])
    if targ_sess_idx >= n_reg_sess:
        raise ValueError(
            f"targ_sess_idx is {targ_sess_idx}, but only {n_reg_sess} "
            "sessions were registered to one another.")

    targ_sess_id = nway_row["sess_ids"][targ_sess_idx]
    targ_sess_n = nway_row["sess_ns"][targ_sess_idx]
    if targ_sess_id == sessid:
        return None # no transform needed

    # get transform from the target session's nway file
    if str(sessid) not in str(nway_match_path):
        raise ValueError(
            "Expected the n-way_match_path to contain the session ID."
            )
    targ_nway_match_path = Path(
        str(nway_match_path).replace(str(sessid), str(targ_sess_id))
        )

    if not Path(targ_nway_match_path).is_file():
        raise RuntimeError(f"Expected to find {targ_nway_match_path} to "
        "retrieve registration transform, but file does not exist.")

    with open(targ_nway_match_path, "r") as f:
        target_nway_metadata = pd.DataFrame().from_dict(json.load(f)["metadata"])

    column_name = f"sess_{sess_n}_to_sess_{targ_sess_n}_transformation_matrix"

    if column_name not in target_nway_metadata.columns:
        raise RuntimeError(
            f"Expected to find {column_name} column in the metadata "
            "dataframe for the target session."
            )

    if len(nway_metadata) != 1:
        raise NotImplementedError(
            "Target session metadata dataframe expected to only have one line."
            )

    target_nway_row = target_nway_metadata.loc[target_nway_metadata.index[0]]
    
    transform_params = np.asarray(target_nway_row[column_name])
        
    return transform_params


#############################################
def apply_registration_transform(nway_match_path, sessid, image, 
                                 targ_sess_idx=0):
    """
    apply_registration_transform(nway_match_path, sessid, image)

    Returns an image transformed using registration transform parameters, saved 
    in the n-way match files. 

    Required args:
        - nway_match_path (Path): full path name of the n-way registration path 
                                  (should be a local path in a directory that 
                                  other session registrations are also stored)
        - sessid (int)          : session ID
        - image (2 or 3D array) : image to transform, with dimensions 
                                  (item x) hei x wid. Only certain datatypes 
                                  are supported by the OpenCV function used, 
                                  e.g. float or uint8.
         
    Optional args:
        - targ_sess_idx (int)   : session that the registration transform 
                                  should be targetted to
                                  default: 0
    
    Returns:
        - registered_image (2 or 3D array) : transformed image, with dimensions 
                                             (item x) hei x wid.
    """
    
    registration_transform_params = get_registration_transform_params(
        nway_match_path, sessid, targ_sess_idx=targ_sess_idx
        )

    if registration_transform_params is None:
        registered_image = image
    else:
        len_image_shape = len(image.shape)
        if len_image_shape == 2:
            image = np.asarray([image])
        elif len_image_shape != 3:
            raise ValueError("image must be a 2D or 3D array.")

        image = np.asarray(image)

        if registration_transform_params.shape != (3, 3):
            raise RuntimeError(
                "registration_transform_params retrieved is expected to have "
                "shape (3, 3), but found shape "
                f"{registration_transform_params.shape}."
                )

        try:
            registered_image = np.asarray(
                [cv2.warpPerspective(
                    sub, 
                    registration_transform_params, 
                    dsize=sub.shape, 
                    flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP
                    )
                    for sub in image]
                )
        except cv2.error as err:
            # try to capture and clarify obscure datatype errors raised by 
            # OpenCV
            if "ifunc" in str(err) or "argument 'src'" in str(err):
                raise RuntimeError(
                    "The following error was raised by OpenCV during image "
                    f"warping: {err}May be due to the use of an unsupported "
                    "datatype. Supported datatypes include uint8, int16, "
                    "uint16, float32, and float64."
                    )
            else:
                raise err

        if len_image_shape == 2:
            registered_image = registered_image[0]

    return registered_image


#############################################
def get_tracking_perm_example_df(nway_match_path, sessid=None, 
                                 idx_after_rem_bad=False):
    """
    get_tracking_perm_example_df(nway_match_path)

    Returns dataframe with tracking permutation example data. (Only a few mice 
    have this data included in their nway-match files.)

    Required args:
        - nway_match_path (Path): full path name of the n-way registration path 
                                  (should be a local path in a directory that 
                                  other session registrations are also stored)         
    Optional args:
        - sessid (int)            : session ID, for error message if file does 
                                    not contain the tracking permutation 
                                    example key
                                    default: None
        - idx_after_rem_bad (bool): if True, the ROI indices (not IDs, however) 
                                    are shifted to as if bad ROIs did not exist
                                    (bad ROIs computed for dF/F only)
                                    default: False

    Returns:
        - nway_tracking_ex_df (pd.DataFrame): dataframe listing ROI tracking 
            matches that were yielded using different session permutations, 
            with columns:
            ['match_level'] (str): 
                whether this permutation produces the 'most' or 'fewest' 
                matches, or whether the row reflects the 'union'
            ['n_total'] (int):
                total number of ROIs for the match level

            if 'match_level' is 'most' or 'fewest' (NaN if 'union')
            ['sess_order'] (list):
                session number order for this permutation
            ['dff_local_missing_roi_idx'] (list):
                indices of ROIs that are included in the final tracked ROIs for 
                the session, but were not identified with this permutation
            ['dff_local_extra_roi_idx'] (list):
                indices of ROIs that are not included in the final tracked ROIs 
                for the session, but were identified with this permutation
            ['sess{}_missing_roi_id'] (list):
                ROI IDs/names corresponding to 'dff_local_missing_roi_idx'
            ['sess{}_extra_roi_id'] (list):
                ROI IDs/names corresponding to 'dff_local_extra_roi_idx'
    """

    if not Path(nway_match_path).is_file():
        raise OSError(f"{nway_match_path} does not exist.")

    with open(nway_match_path, "r") as f:
        nway_dict = json.load(f)
    
    match_key = "match_perm_examples"
    if match_key not in nway_dict.keys():
        sess_str = "" if sessid is None else f" for session {sessid}"
        raise RuntimeError(f"nway-match file{sess_str} does not contain "
            f"example tracking permutation data under {match_key}."
            )
    
    nway_tracking_ex_df = pd.DataFrame().from_dict(nway_dict[match_key])

    # check that missing ROI indices are all tracked, and extra ROI indices are 
    # all untracked for the session
    rois_df = pd.DataFrame().from_dict(nway_dict["rois"])
    for col in nway_tracking_ex_df.columns:
        if "roi_idx" not in col:
            continue
        targ_vals = rois_df["dff-ordered_roi_index"].tolist()
        for row_idx in nway_tracking_ex_df.index:
            if nway_tracking_ex_df.loc[row_idx, "match_level"] == "union":
                continue

            roi_idxs = nway_tracking_ex_df.loc[row_idx, col]
            for n in roi_idxs:
                if n in targ_vals and "extra" in col:
                    raise RuntimeError(
                        "Some ROIs identified as 'extra' are in fact tracked."
                        )
                elif n not in targ_vals and "missing" in col:
                    raise RuntimeError(
                        "Some ROIs identified as 'missing' are not in fact "
                        "tracked."
                        )

    # shift ROI indices to as if bad ROIs did not exist
    if idx_after_rem_bad:
        bad_rois_df = pd.DataFrame().from_dict(nway_dict["bad_rois"])
        bad_rois = bad_rois_df["dff_local_bad_roi_idx"].values

        idx_cols = ["dff_local_missing_roi_idx", "dff_local_extra_roi_idx"]
        for row_idx in nway_tracking_ex_df.index:
            if nway_tracking_ex_df.loc[row_idx, "match_level"] == "union":
                continue
            for col in idx_cols:
                roi_idxs = nway_tracking_ex_df.loc[row_idx, col]
                if len(roi_idxs) == 0:
                    continue
                
                # shift indices to ignore bad ROIs
                adj_roi_idxs = []
                for n in roi_idxs:
                    adj_roi_idxs.append(n - np.sum(bad_rois < n))
                nway_tracking_ex_df.at[row_idx, col] = adj_roi_idxs

    return nway_tracking_ex_df


#############################################
def _warn_nans_diff_thr(run, min_consec=5, n_pre_existing=None, sessid=None):
    """
    _warn_nans_diff_thr(run)

    Checks for NaNs in running velocity, and logs a warning about the total 
    number of NaNs, and the consecutive NaNs. Optionally indicates the number 
    of pre-existing NaNs, versus number of NaNs resulting from the difference 
    threshold. 

    Required args:
        - run (1D array): array of running velocities in cm/s

    Optional args:
        - min_consec (num)    : minimum number of consecutive NaN running 
                                values to warn aboout
                                default: 5
        - n_pre_existing (num): number of pre-existing NaNs (before difference 
                                thresholding was used)
                                default: None
        - sessid (int)        : session ID to include in the log or error
                                default: None 
    """

    n_nans = np.sum(np.isnan(run))

    if n_nans == 0:
        return

    split_str = ""
    if n_pre_existing is not None:
        if n_pre_existing == n_nans:
            split_str = " (in pre-processing)"
        elif n_pre_existing == 0:
            split_str = " (using diff thresh)"
        else:
            split_str = (f" ({n_pre_existing} in pre-processing, "
                f"{n_nans - n_pre_existing} more using diff thresh)")

    mask = np.concatenate(([False], np.isnan(run), [False]))
    idx = np.nonzero(mask[1 : ] != mask[ : -1])[0]
    n_consec = np.sort(idx[1 :: 2] - idx[ :: 2])[::-1]

    n_consec_above_min_idx = np.where(n_consec > min_consec)[0]
    
    n_consec_str = ""
    if len(n_consec_above_min_idx) > 0:
        n_consec_str = ", ".join(
            [str(n) for n in n_consec[n_consec_above_min_idx]])
        n_consec_str = (f"\n{TAB}This includes {n_consec_str} consecutive "
            "dropped running values.")

    prop = n_nans / len(run)
    sessstr = "" if sessid is None else f"Session {sessid}: "
    
    logger.warning(f"{sessstr}{n_nans} dropped running frames "
        f"(~{prop * 100:.1f}%){split_str}.{n_consec_str}", 
        extra={"spacing": TAB})

    return


#############################################
def nan_large_run_differences(run, diff_thr=50, warn_nans=True, 
                              drop_tol=0.0003, sessid=None):
    """
    nan_large_run_differences(run)

    Returns running velocity with outliers replaced with NaNs.

    Required args:
        - run (1D array): array of running velocities in cm/s

    Optional args:
        - diff_thr (int)    : threshold of difference in running velocity to 
                              identify outliers
                              default: 50
        - warn_nans (bool)  : if True, a warning is logged 
                              default: True
        - drop_tol (num)    : the tolerance for proportion running frames 
                              dropped. A warning is produced only if this 
                              condition is not met. 
                              default: 0.0003 
        - sessid (int)      : session ID to include in the log or error
                              default: None 

    Returns:
        - run (1D array): updated array of running velocities in cm/s
    """

    # temorarily remove preexisting NaNs (to be reinserted after)
    original_length = len(run)
    not_nans_idx = np.where(~np.isnan(run))[0]
    run = run[not_nans_idx]
    n_pre_existing = original_length - len(run)

    run_diff = np.diff(run)
    out_idx = np.where((run_diff < -diff_thr) | (run_diff > diff_thr))[0]
    at_idx = -1
    for idx in out_idx:
        if idx > at_idx:
            if idx == 0:
                # in case the first value is completely off
                comp_val = 0
                if np.absolute(run[0]) > diff_thr:
                    run[0] = np.nan
            else:
                comp_val = run[idx]
            while np.absolute(run[idx + 1] - comp_val) > diff_thr:
                run[idx + 1] = np.nan
                idx += 1
            at_idx = idx

    # reinsert pre-existing NaNs
    prev_run = copy.deepcopy(run)
    run = np.full(original_length, np.nan)
    run[not_nans_idx] = prev_run

    prop_nans = np.sum(np.isnan(run)) / len(run)
    if warn_nans and prop_nans > drop_tol:
        _warn_nans_diff_thr(
            run, min_consec=5, n_pre_existing=n_pre_existing, sessid=sessid
            )

    return run


#############################################
def load_run_data_nwb(sess_files, diff_thr=50, drop_tol=0.0003, sessid=None):
    """
    load_run_data_nwb(sess_files)

    Returns pre-processed running velocity from NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Optional args:
        - diff_thr (int): threshold of difference in running velocity to 
                          identify outliers
                          default: 50
        - drop_tol (num): the tolerance for proportion running frames 
                          dropped. A warning is produced only if this 
                          condition is not met. 
                          default: 0.0003 
        - sessid (int)  : session ID to include in the log or error
                          default: None 
    Returns:
        - run_velocity (1D array): array of running velocities in cm/s for each 
                                   recorded stimulus frames

    """

    behav_file = sess_file_util.select_nwb_sess_path(sess_files, behav=True)

    with pynwb.NWBHDF5IO(str(behav_file), "r") as f:
        nwbfile_in = f.read()
        behav_module = nwbfile_in.get_processing_module("behavior")
        main_field = "BehavioralTimeSeries"
        data_field = "running_velocity"
        try:
            behav_time_series = behav_module.get_data_interface(
                main_field).get_timeseries(data_field)
        except KeyError as err:
            raise KeyError(
                "Could not find running velocity data in behavioral time "
                f"series for {behav_module} due to: {err}"
                )
        
        run_velocity = np.asarray(behav_time_series.data)

    run_velocity = nan_large_run_differences(
        run_velocity, diff_thr, warn_nans=True, drop_tol=drop_tol, 
        sessid=sessid
        )

    return run_velocity


#############################################
def load_run_data(stim_dict, stim_sync_h5, filter_ks=5, diff_thr=50, 
                  drop_tol=0.0003, sessid=None):
    """
    load_run_data(stim_dict, stim_sync_h5)

    Returns running velocity with outliers replaced with NaNs, and median 
    filters the data.

    Required args:
        - stim_dict (Path or dict): stimulus dictionary or path to dictionary,
                                   containing stimulus information
        - stim_sync_h5 (Path)     : stimulus synchronization file. 

    Optional args:
        - filter_ks (int)   : kernel size to use in median filtering the 
                              running velocity (0 to skip filtering).
                              default: 5
        - diff_thr (int)    : threshold of difference in running velocity to 
                              identify outliers
                              default: 50
        - drop_tol (num)    : the tolerance for proportion running frames 
                              dropped. A warning is produced only if this 
                              condition is not met. 
                              default: 0.0003 
        - sessid (int)      : session ID to include in the log or error
                              default: None 

    Returns:
        - run_velocity (1D array): array of running velocities in cm/s for each 
                                   recorded stimulus frames

    """

    run_kwargs = {
        "stim_sync_h5": stim_sync_h5,
        "filter_ks"   : filter_ks,
    }

    if isinstance(stim_dict, dict):
        run_kwargs["stim_dict"] = stim_dict        
    elif isinstance(stim_dict, (str, Path)):
        run_kwargs["stim_pkl"] = stim_dict
    else:
        raise TypeError(
            "'stim_dict' must be a dictionary or a path to a pickle."
            )

    run_velocity = sess_sync_util.get_run_velocity(**run_kwargs)

    run_velocity = nan_large_run_differences(
        run_velocity, diff_thr, warn_nans=True, drop_tol=drop_tol, 
        sessid=sessid
        )

    return run_velocity


#############################################
def load_pup_data_nwb(sess_files):
    """
    load_pup_data_nwb(sess_files)

    Returns pre-processed pupil data from NWB files. 

    Required args:
        - sess_files (Path): full path names of the session files

    Returns:
        - pup_data_df (pd DataFrame): pupil data dataframe with columns:
            - frames (int)        : frame number
            - pup_diam (float)    : median pupil diameter in pixels
    """

    behav_file = sess_file_util.select_nwb_sess_path(sess_files, behav=True)

    with pynwb.NWBHDF5IO(str(behav_file), "r") as f:
        nwbfile_in = f.read()
        behav_module = nwbfile_in.get_processing_module("behavior")
        main_field = "PupilTracking"
        data_field = "pupil_diameter"
        try:
            behav_time_series = behav_module.get_data_interface(
                main_field).get_timeseries(data_field)
        except KeyError as err:
            raise KeyError(
                "Could not find pupil diameter data in behavioral time "
                f"series for {behav_module} due to: {err}"
                )
        
        pup_data = np.asarray(behav_time_series.data)

    pup_data_df = pd.DataFrame()
    pup_data_df["pup_diam"] = pup_data / sess_sync_util.MM_PER_PIXEL

    pup_data_df.insert(0, "frames", value=range(len(pup_data_df)))

    return pup_data_df  


#############################################
def load_pup_data(pup_data_h5, time_sync_h5):
    """
    load_pup_data(pup_data_h5, time_sync_h5)

    If it exists, loads the pupil tracking data. Extracts pupil diameter
    and position information in pixels, converted to two-photon frames.

    If it doesn't exist or several are found, raises an error.

    Required args:
        - pup_data_h5 (Path or list): path to the pupil data h5 file
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - pup_data (pd DataFrame): pupil data dataframe with columns:
            - frames (int)        : frame number
            - pup_diam (float)    : median pupil diameter in pixels
            - pup_center_x (float): pupil center position for x at 
                                    each pupil frame in pixels
            - pup_center_y (float): pupil center position for y at 
                                    each pupil frame in pixels
    """

    if pup_data_h5 == "none":
        raise OSError("No pupil data file found.")
    elif isinstance(pup_data_h5, list):
        raise OSError("Many pupil data files found.")

    columns = ["nan_diam", "nan_center_x", "nan_center_y"]
    orig_pup_data = pd.read_hdf(pup_data_h5).filter(items=columns).astype(float)
    nan_pup = (lambda name : name.replace("nan_", "pup_") 
        if "nan" in name else name)
    orig_pup_data = orig_pup_data.rename(columns=nan_pup)

    with h5py.File(time_sync_h5, "r") as f:
        twop_timestamps = f["twop_vsync_fall"][:]

        mean_twop_fps = 1.0 / np.mean(np.diff(twop_timestamps))
        delay = int(np.round(mean_twop_fps * 0.1))
        eye_alignment = f["eye_tracking_alignment"][:].astype(int) + delay

    pup_data = pd.DataFrame()
    last_keep = np.where(eye_alignment < len(orig_pup_data))[0][-1]
    for col in orig_pup_data.columns:
        pup_data[col] = orig_pup_data[col].to_numpy()[
            eye_alignment[: last_keep + 1]
            ]

    pup_data.insert(0, "frames", value=range(len(pup_data)))

    return pup_data  


#############################################
def load_stimulus_images_nwb(sess_files, template_name="gabors", frame_ns=[0]):
    """
    load_stimulus_images_nwb(sess_files)

    Returns the stimulus images for the requested template, and frames. 

    Required args:
        - sess_files (Path): full path names of the session files

    Optional args:
        - template_name (str): name of the stimulus template
                               default: "gabors" 
        - frame_ns (list)    : frame numbers (must be unique and in increasing 
                               order)
                               default: frame_ns

    Returns:
        - stim_images (list): list of stimulus images (grayscale)
                              (height x width x channel (1))
    """

    stim_file = sess_file_util.select_nwb_sess_path(sess_files, stim=True)

    frame_ns = gen_util.list_if_not(frame_ns)
    unique_bool = (len(np.unique(frame_ns)) == len(frame_ns))
    increasing_bool = (np.sort(frame_ns) == np.asarray(frame_ns)).all()

    if not (unique_bool and increasing_bool):
        raise ValueError(
            "frame_ns must be provided in strictly increasing order, and "
            "comprise only unique values."
            )

    with pynwb.NWBHDF5IO(str(stim_file), "r") as f:
        nwbfile_in = f.read()
        try:
            stim_template = nwbfile_in.get_stimulus_template(template_name)

            n_frames = stim_template.data.shape[0]
            if max(frame_ns) > n_frames - 1:
                raise ValueError(
                    "Some of the frames requested go beyond the number of "
                    "frames recorded for this template."
                    )
            # loading the images can be slow
            template_images = stim_template.data[frame_ns]

        except KeyError as err:
            raise KeyError(
                f"Could not find frames for '{template_name}' stimulus "
                f"template in {stim_file} due to: {err}"
                )
        
    template_images = list(template_images)

    return template_images


#############################################
def load_pup_sync_h5_data(pup_video_h5):
    """
    load_pup_sync_h5_data(pup_video_h5)

    Returns pupil synchronization information.

    Required args:
        - pup_video_h5 (Path): path to the pupil video h5 file

    Returns:
        - pup_fr_interv (1D array): interval in sec between each pupil 
                                    frame
    """

    with h5py.File(pup_video_h5, "r") as f:
        pup_fr_interv = f["frame_intervals"][()].astype("float64")

    return pup_fr_interv


#############################################
def load_beh_sync_h5_data(time_sync_h5):
    """
    load_beh_sync_h5_data(time_sync_h5)

    Returns behaviour synchronization information.

    Required args:
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - twop2bodyfr (1D array)  : body-tracking video (video-0) frame 
                                    numbers for each 2p frame
        - twop2pupfr (1D array)   : eye-tracking video (video-1) frame 
                                    numbers for each 2p frame
        - stim2twopfr2 (1D array) : 2p frame numbers for each stimulus 
                                    frame, as well as the flanking
                                    blank screen frames (second 
                                    version, very similar to stim2twopfr 
                                    with a few differences)
    """

    with h5py.File(time_sync_h5, "r") as f:
        twop2bodyfr  = f["body_camera_alignment"][()].astype("int")
        twop2pupfr   = f["eye_tracking_alignment"][()].astype("int")
        stim2twopfr2 = f["stimulus_alignment"][()].astype("int")

    return twop2bodyfr, twop2pupfr, stim2twopfr2


#############################################
def load_sync_h5_data(pup_video_h5, time_sync_h5):
    """
    load_sync_h5_data(pup_video_h5, time_sync_h5)

    Returns pupil and behaviour synchronization information.

    Required args:
        - pup_video_h5 (Path): path to the pupil video h5 file
        - time_sync_h5 (Path): path to the time synchronization hdf5 file

    Returns:
        - pup_fr_interv (1D array): interval in sec between each pupil 
                                    frame
        - twop2bodyfr (1D array)  : body-tracking video (video-0) frame 
                                    numbers for each 2p frame
        - twop2pupfr (1D array)   : eye-tracking video (video-1) frame 
                                    numbers for each 2p frame
        - stim2twopfr2 (1D array) : 2p frame numbers for each stimulus 
                                    frame, as well as the flanking
                                    blank screen frames (second 
                                    version, very similar to stim2twopfr 
                                    with a few differences)
    """

    pup_fr_interv = load_pup_sync_h5_data(pup_video_h5)

    twop2bodyfr, twop2pupfr, stim2twopfr2 = load_beh_sync_h5_data(time_sync_h5)

    return pup_fr_interv, twop2bodyfr, twop2pupfr, stim2twopfr2


#############################################
def modify_visflow_segs(stim_df, runtype="prod"):
    """
    modify_visflow_segs(stim_df)

    Returns stim_df with visual flow segment numbers modified to ensure that
    they are different for the two visual flow stimuli in the production data.

    Required args:
        - stim_df (pd DataFrame): stimlus alignment dataframe with columns:
                                    "stimtype", "unexp", "stim_seg", "gabfr", 
                                    "gab_ori", "gabk", "visflow_dir", 
                                    "visflow_size", "start_twop_fr", 
                                    "end_twop_fr", "num_twop_fr"

    Optional args:
        - runtype (str): runtype
                         default: "prod"

    Returns:
        - stim_df (pd DataFrame): modified dataframe
    """

    if runtype != "prod":
        return stim_df

    stim_df = copy.deepcopy(stim_df)

    visflow_st_fr = gen_util.get_df_vals(
        stim_df, "stimtype", "v", "start_twop_fr", unique=False)
    visflow_num_fr = np.diff(visflow_st_fr)
    num_fr = gen_util.get_df_vals(
        stim_df, "stimtype", "v", "num_twop_fr", unique=False)[:-1]
    break_idx = np.where(num_fr != visflow_num_fr)[0]
    n_br = len(break_idx)
    if n_br != 1:
        raise RuntimeError("Expected only one break in the visual flow "
            f"stimulus, but found {n_br}.")
    
    # last start frame and seg for the first visual flow stim
    last_fr1 = visflow_st_fr[break_idx[0]] 
    last_seg1 = gen_util.get_df_vals(
        stim_df, ["stimtype", "start_twop_fr"], ["v", last_fr1], "stim_seg")[0]
    
    seg_idx = (
        (stim_df["stimtype"] == "v") & 
        (stim_df["start_twop_fr"] > last_fr1)
        )

    new_idx = stim_df.loc[seg_idx]["stim_seg"] + last_seg1 + 1
    stim_df = gen_util.set_df_vals(
        stim_df, seg_idx, "stim_seg", new_idx, in_place=True
        )

    return stim_df


#############################################
def load_sess_stim_seed(stim_dict, runtype="prod"):
    """
    load_sess_stim_seed(stim_dict)

    Returns session's stimulus seed for this session. Expects all stimuli 
    stored in the session's stimulus dictionary to share the same seed.

    Required args:
        - stim_dict (dict): stimlus dictionary

    Optional args:
        - runtype (str): runtype
                         default: "prod"

    Returns:
        - seed (int): session's stimulus seed
    """

    if runtype == "pilot":
        stim_param_key = "stimParams"
        sess_param_key = "subj_params"
    elif runtype == "prod":
        stim_param_key = "stim_params"
        sess_param_key = "session_params"
    else:
        gen_util.accepted_values_error("runtype", runtype, ["pilot", "prod"])

    seeds = []
    for stimulus in stim_dict["stimuli"]:
        seeds.append(stimulus[stim_param_key][sess_param_key]["seed"])
    
    if np.max(seeds) != np.min(seeds):
        raise RuntimeError("Unexpectedly found different seeds for different "
        "stimuli for this session.")
    
    seed = seeds[0]

    return seed

    