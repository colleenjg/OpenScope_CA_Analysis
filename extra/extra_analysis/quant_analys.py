"""
quant_analys.py

This module contains functions to run quantile analyses on data
generated by the Allen Institute OpenScope experiments for the Credit 
Assignment Project.

Authors: Colleen Gillon

Date: October, 2018

Note: this code uses python 3.7.

"""

import copy
import warnings

import numpy as np
import pandas as pd

from util import gen_util, logger_util, math_util, rand_util
from sess_util import sess_ntuple_util, sess_gen_util


TAB = "    "


logger = logger_util.get_module_logger(name=__name__)


#############################################
def define_transition_baseline(stimtype="visflow", gabfr=3, baseline=0.1, pre=0, 
                               post=1.5):
    """
    define_transition_baseline()

    Returns transition baseline times based on stimulus type and times. 
    
    For gabors, the baseline is before onset of the earliest D/U frame in the 
    stimulus period. If there are no D/U frames in the stimulus period 
    (including if the stimulus period ends at a first D/U frame), the 
    closest preceeding D/U frame is chosen. 

    For visual flow, the baseline is before 0 seconds reference point.

    Optional args:
        - stimtype (str)  : stimulus ("visflow", "gabors", "both")
                            default: "visflow"
        - gabfr (int)     : gabor frame at which stimulus period start 
                            (0, 1, 2, 3) (or to include, for GLM)
                            default: 0
        - baseline (float): baseline length (in sec)
                            default: 0.1
        - pre (float)     : range of frames included in stimulus period before 
                            each reference frame (in s)
                            default: 0 (in sec)
        - post (float)    : range of frames included in stimulus period after 
                            each reference frame (in s)
                            default: 1.5 (in sec)

    Returns:
        - pre (float)     : range of frames in stimulus period before each 
                            reference frame included in baseline (in s)
        - post (float)    : range of frames in stimulus period after each 
                            reference frame included in baseline (in s)
    """

    if stimtype in ["visflow", "both"]: # same as provided pre
        base_pre = baseline
    elif stimtype == "gabors":
        sec_per, targ_fr, full = 0.3, 3, 1.5
        # offset from 0, first from pre
        offset = (sec_per * (targ_fr - gabfr) + pre) % full - pre
        # if beyond stimulus range, move to before stimulus period
        if offset >= post:
            offset -= full
        # reverses base_pre: if positive -> before 0, and v.v.
        base_pre = baseline - offset

    base_post = baseline - base_pre
    
    return base_pre, base_post


#############################################
def quant_segs(stim, stimpar, n_quants=4, qu_idx="all", unexp="any", 
               empty_ok=False, remconsec=False, by_exp_len=False):
    """
    quant_segs(stim, stimpar)

    Returns segments split into quantiles.
    
    Required args:
        - stim (Stim)      : stim object
        - stimpar (StimPar): named tuple containing stimulus parameters

    Optional args:
        - n_quants (int)      : number of quantiles to split data into
                                default: 4
        - qu_idx (str or list): indices of quantiles to retain
                                default: "all"
        - unexp (int or list)  : unexpected values to include (e.g., 0 or 1)
                                default: "any"
        - empty_ok (bool)     : if True, catches error if no segments respond 
                                to criteria and returns empty qu_segs and 
                                qu_counts = 0.
                                default: False 
        - remconsec (bool)    : if True, consecutive segments are removed
                                default: False
        - by_exp_len (bool)  : if True, consecutive segments are removed and
                                the number of consecutive segments 
                                corresponding to each retained segment is also 
                                returned
                                default: False

    Returns:
        - qu_segs (list)  : list of sublists for each quantile, each containing 
                            segment numbers for that quantile
        - qu_counts (list): list of number of segments in each quantile
        if by_exp_len:
        - qu_n_consec (list): list of sublists for each quantile, each
                                containing the number of consecutive segments
                                corresponding to the values in qu_segs
    """

    if qu_idx == "all":
        qu_idx = list(range(n_quants))
    else:
        qu_idx = gen_util.pos_idx(qu_idx, n_quants)
    
    # get all seg values (for all gabor frames and orientations)
    try:
        all_segs = stim.get_segs_by_criteria(gabk=stimpar.gabk, 
            visflow_dir=stimpar.visflow_dir, visflow_size=stimpar.visflow_size, 
            by="seg")
    except RuntimeError as err:
        if empty_ok and "fit these criteria" in str(err):
            all_segs = []
        else:
            raise err

    if len(all_segs):
        all_segs.append(np.max(all_segs) + 1)
        qu_percs = [
            np.percentile(all_segs, q=perc) 
            for perc in np.linspace(0, 100, n_quants + 1)
            ]
    else:
        qu_percs = [0] * (n_quants + 1) # dummy list

    # get all seg values
    try:
        all_segs = stim.get_segs_by_criteria(gabfr=stimpar.gabfr,
            gabk=stimpar.gabk, gab_ori=stimpar.gab_ori,
            visflow_dir=stimpar.visflow_dir, visflow_size=stimpar.visflow_size,
            unexp=unexp, by="seg", remconsec=remconsec)
    except RuntimeError as err:
        if empty_ok and  "fit these criteria" in str(err):
            all_segs = []
        else:
            raise err                                     
    
    if by_exp_len:
        all_segs, n_consec = gen_util.consec(all_segs)
        qu_n_consec = []

    # get seg ranges for each quantile [[start, end], [start, end], etc.] 
    qu_segs, qu_counts = [], []
    for q, qu_perc in enumerate(qu_percs[:-1]):
        if q not in qu_idx:
            continue
        qu_segs.append(
            [seg for seg in all_segs 
            if (seg >= qu_perc and seg < qu_percs[q + 1])]
            )
        qu_counts.extend([len(qu_segs[-1])])
        if by_exp_len: # also include lengths
            qu_n_consec.append(
                [n for i, n in enumerate(n_consec) 
                if (all_segs[i] >= qu_perc and 
                    all_segs[i] < qu_percs[q + 1])
                    ]
                )

    empty_quants = (np.min([len(segs) for segs in qu_segs]) == 0)
    if not empty_ok and empty_quants:
        raise RuntimeError("Some quantiles are empty.")

    if by_exp_len:
        return qu_segs, qu_counts, qu_n_consec
    else:
        return qu_segs, qu_counts


#############################################
def samp_quant_segs(qu_segs, seg_pre=0, seg_post=0, randst=None):
    """
    samp_quant_segs(qu_segs, seg_pre, seg_post)

    Returns segments sampled from each series of consecutive segments, still 
    split into quantiles.

    Required args:
        - qu_segs (list)  : list of sublists for each quantile, each containing 
                            segment numbers for that quantile

    Optional args:
        - seg_pre (int) : minimum difference between the sampled segment number 
                          and the lowest segment number in each consecutive 
                          series  
                          default: 0
        - seg_post (int): minimum difference between the sampled segment number 
                          and the highest segment number in each consecutive 
                          series  
                          default: 0
        - randst (int)  : random state or seed for sampling segments
                          default: None

    Returns:
        - qu_segs (list)  : list of sublists for each quantile, each containing 
                            the sampled segment numbers for that quantile
        - qu_counts (list): list of number of segments in each quantile

    """
    seg_pre = int(np.around(seg_pre))
    seg_post = int(np.around(seg_post))

    qu_segs_flat = [seg for segs in qu_segs for seg in segs]

    if min(np.diff(qu_segs_flat)) not in [1, 5]:
        raise ValueError(
            "No consecutive segments (1 or 5 interval) found in qu_segs."
            )

    all_segs, n_consec = gen_util.consec(qu_segs_flat, smallest=True)

    randst = rand_util.get_np_rand_state(randst)

    samp_segs = []
    i = 0
    for _, n in zip(all_segs, n_consec):
        qu_sub = qu_segs_flat[i : i + n]
        min_seg = min(qu_sub) + seg_pre
        max_seg = max(qu_sub) - seg_post
        if min_seg < max_seg:
            qu_samp = [seg for seg in qu_sub if seg in range(min_seg, max_seg)]
            samp_seg = randst.choice(qu_samp)
            samp_segs.append(samp_seg)
        i += n
    
    qu_samp_segs = []
    qu_counts = []
    for sub_segs in qu_segs:
        min_seg = min(sub_segs)
        max_seg = max(sub_segs)
        qu_samps = [seg for seg in samp_segs if seg in range(min_seg, max_seg)]
        qu_samp_segs.append(qu_samps)
        qu_counts.append(len(qu_samps))

    return qu_samp_segs, qu_counts


#############################################
def trace_stats_by_qu(stim, qu_segs, pre, post, analyspar, byroi=True, 
                      integ=False, ret_arr=False, nan_empty=False, 
                      baseline=None, datatype="roi"):
    """
    trace_stats_by_qu(stim, qu_seg, pre, post, analyspar)

    Returns trace statistics for the quantiles of interest. If ret_arr, also
    returns trace data arrays.

    Required args:
        - stim (Stim object)   : stim object
        - qu_segs (dict)       : list of sublists for each quantile, each 
                                 containing segment numbers for that quantile
        - pre (num)            : range of frames to include before each frame 
                                 reference (in s)
        - post (num)           : range of frames to include after each frame 
                                 reference (in s)
        - analyspar (AnalysPar): named tuple containing analysis parameters
    
    Optional args:
        - byroi (bool)    : If datatype is "roi", if True, returns statistics 
                            for each ROI. If False, returns statistics 
                            across ROIs.
                            default: True
        - integ (bool)    : if True, dF/F is integrated over sequences
                            default: False
        - ret_arr (bool)  : if True, data arrays are returned also
                            default: False
        - nan_empty (bool): if a quantile is empty, returns NaN arrays instead
                            of an error (1 sequence, for qu_array) 
                            default: False
        - baseline (num)  : number of seconds to use as baseline. If None,
                            data is not baselined.
                            default: None
        - datatype (str)  : datatype, i.e. ROIs or running
                            default: "roi"

    Returns:
        - xran (1D array)          : time values for the 2p frames (None if 
                                     integ)
        - qu_stats (2 to 4D array) : trace data statistics, structured as:
                                         quantiles x
                                         stats (me, err) x
                                         (ROIs if byroi x)
                                         (frames if not integ)
        if ret_arr, also:
        - qu_array (list)          : list per quantile of 1-3D arrays of trace 
                                     data structured as:
                                        (ROIs x) sequences 
                                        (x frames if not integ)
    """
    
    if datatype == "roi" and (stim.sess.only_tracked_rois != analyspar.tracked):
        raise RuntimeError(
            "stim.sess.only_tracked_rois should match analyspar.tracked."
            )

    qu_stats, qu_array = [], []
    xran = None
    for segs in qu_segs:
        rep_nan = False
        for _ in range(2): # allows retrying if nan_empty is True
            try:
                if datatype == "roi":
                    twop_fr = stim.get_fr_by_seg(
                        segs, start=True, fr_type="twop")["start_frame_twop"]
                    trace_df = stim.get_roi_stats_df(twop_fr, pre, post, 
                        byroi=byroi, fluor=analyspar.fluor, 
                        rem_bad=analyspar.rem_bad, 
                        stats=analyspar.stats, error=analyspar.error,
                        integ=integ, ret_arr=ret_arr, scale=analyspar.scale,
                        baseline=baseline)
                elif datatype == "run":
                    stim_fr = stim.get_fr_by_seg(
                        segs, start=True, fr_type="stim")["start_frame_stim"]
                    trace_df = stim.get_run_stats_df(stim_fr, pre, post, 
                        rem_bad=analyspar.rem_bad,
                        stats=analyspar.stats, error=analyspar.error,
                        integ=integ, ret_arr=ret_arr, scale=analyspar.scale,
                        baseline=baseline)
                else:
                    gen_util.accepted_values_error(
                        "datatype", datatype, ["roi", "run"])
                break # break out of for loop if successful
            except Exception as err: # RuntimeError or ValueError
                empty = ("No frames" in str(err) or "No segments" in str(err))
                if nan_empty and empty:
                    segs = [10]     # dummy segment to use
                    rep_nan = True # later, replace values with NaNs
                else:
                    raise err

        if not integ:
            xran = trace_df.index.unique("time_values").to_numpy()
        
        # array: stats [me, err] (x ROI) (x frames)
        # (catch performance warning for unsorted large dataframes)
        msg, categ = ["indexing past lexsort", pd.errors.PerformanceWarning]
        with gen_util.TempWarningFilter(msg, categ):
            trace_stats = gen_util.reshape_df_data(
                trace_df.loc["stats", ], squeeze_cols=True)

        if datatype == "roi":
            if not byroi:
                trace_stats = trace_stats.squeeze(0)
            else:
                trace_stats = trace_stats.transpose(
                    1, 0, *range(len(trace_stats.shape))[2:])
        if rep_nan: # replace dummy values with NaNs
            trace_stats = np.full_like(trace_stats, np.nan)
        qu_stats.append(trace_stats)
        if ret_arr:
            trace_array = gen_util.reshape_df_data(
                trace_df.loc["data", ], squeeze_cols=True)
            if rep_nan: # replace dummy values with NaNs
                trace_array = np.full_like(trace_array, np.nan)
            qu_array.append(trace_array)

    qu_stats = np.asarray(qu_stats)

    if ret_arr:
        return xran, qu_stats, qu_array
    else:
        return xran, qu_stats


#############################################
def trace_stats_by_qu_sess(sessions, analyspar, stimpar, n_quants=4, 
                           qu_idx="all", byroi=True, by_exp=False, integ=False, 
                           ret_arr=False, nan_empty=False, lock="no", 
                           baseline=None, datatype="roi", randst=None):
    """
    trace_stats_by_qu_sess(sessions, analyspar, stimpar)

    Returns trace statistics for the quantiles of interest for each
    session and unexpected value, for the datatype of interest.

    Required args:
        - sessions (list)      : list of Session objects
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        
    Optional args:
        - n_quants (int)      : number of quantiles to divide sessions into
                                default: 4
        - qu_idx (str or list): indices of quantiles to retain
                                default: "all"
        - byroi (bool)        : If datatype is "roi", if True, returns 
                                statistics for each ROI. If False, returns 
                                statistics across ROIs.
                                default: True
        - by_exp (bool)     : if True, quantiles are separated into unexpected 
                                and expected groups.
                                default: False
        - integ (bool)        : if True, dF/F is integrated over sequences
                                default: False
        - ret_arr (bool)      : if True, data arrays are returned also
                                default: False
        - nan_empty (bool)    : if a quantile is empty, return NaN arrays 
                                (avoids an error)
                                default: False
        - lock (bool)         : if "unexp", "exp", "exp_samp", only the first 
                                unexpected or expected segments are retained.
                                If "both"
                                (by_exp is ignore). 
                                default: False
        - baseline (num)      : number of seconds to use as baseline. If None,
                                data is not baselined.
                                default: None
        - datatype (str)      : datatype, i.e. ROIs or running
                                default: "roi"
        - randst (int)        : random state or seed for sampling segments
                                default: None

    Returns:
        - xrans (list)             : time values for the 2p frames (None if 
                                     integ), for each session
        - all_stats (list)         : list of 2 to 5D arrays of trace data 
                                     statistics for each session, structured as:
                                         (unexp if by_exp x)
                                         quantiles x
                                         stats (me, err) x
                                         (ROIs if byroi x)
                                         (frames if not integ)
        - all_counts (nested list) : list of number of sequences, 
                                     structured as:
                                        sess 
                                        x (unexp if by_exp or lock is "both") 
                                        x quantiles
        if ret_arr:
        - all_arrays (nested lists): list of data trace arrays, structured as:
                                        session (x unexp if by_exp) x quantile 
                                        of 1 to 3D arrays: 
                                            (ROI x) sequences 
                                            (x frames if not integ)
    """

    incr_unexp_ori = False
    if stimpar.stimtype == "gabors":
        if by_exp and isinstance(stimpar.gab_ori, int):
            if stimpar.gabfr == 3:
                incr_unexp_ori = True
                warnings.warn(
                    "Incrementing orientation for unexpected segments to "
                    "ensure data is paired for by_exp split.", 
                    category=RuntimeWarning, stacklevel=1
                )

    shift_gab_segs = False
    remconsec, sample = False, False
    unexp_vals = ["any"]
    if lock in ["unexp", "exp", "both"]:
        remconsec = True
        unexp_vals = [1, 0]
        if lock == "exp":
            unexp_vals = [0]
        elif lock == "unexp":
            unexp_vals = [1]
        if stimpar.stimtype == "gabors" and stimpar.gabfr not in ["any", "all"]:
            shift_gab_segs = True
            orig_gabfr = stimpar.gabfr
            stimpar = sess_ntuple_util.get_modif_ntuple(stimpar, "gabfr", "any")
    elif lock == "exp_samp":
        remconsec, sample = False, True
        unexp_vals = [0]
    elif by_exp:
        unexp_vals = [0, 1]    

    all_counts, all_stats, all_arrays = [], [], []
    xrans = []
    for sess in sessions:
        stim = sess.get_stim(stimpar.stimtype)
        sess_counts, sess_stats, sess_arrays = [], [], []
        for unexp in unexp_vals:
            stimpar_use = stimpar
            if incr_unexp_ori and unexp == 1:
                incr_ori = sess_gen_util.get_unexp_gab_ori(stimpar.gab_ori)
                stimpar_use = sess_ntuple_util.get_modif_ntuple(
                    stimpar, "gab_ori", incr_ori
                    )

            qu_segs, qu_counts = quant_segs(
                stim, stimpar_use, n_quants, qu_idx, unexp, empty_ok=nan_empty, 
                remconsec=remconsec)
            if shift_gab_segs: # shift to requested gabor frame
                qu_segs = [[s + orig_gabfr for s in segs] for segs in qu_segs]
            if sample:
                pre_seg = stimpar.pre / stim.seg_len_s
                post_seg = stimpar.post / stim.seg_len_s
                qu_segs, qu_counts = samp_quant_segs(
                    qu_segs, pre_seg, post_seg, randst=randst
                    )
            sess_counts.append(qu_counts)
            trace_info = trace_stats_by_qu(
                stim, qu_segs, stimpar.pre, stimpar.post, analyspar, 
                byroi=byroi, integ=integ, ret_arr=ret_arr, nan_empty=nan_empty, 
                baseline=baseline, datatype=datatype)
            sess_stats.append(trace_info[1])
            if ret_arr:
                sess_arrays.append(trace_info[2])
        xrans.append(trace_info[0])
        if len(unexp_vals) > 1:
            sess_stats = np.asarray(sess_stats)
        else:
            sess_stats = np.asarray(sess_stats[0]) # list of length 1
            sess_counts = sess_counts[0]
            if ret_arr:
                sess_arrays = sess_arrays[0] # list of length 1
        all_counts.append(sess_counts)
        all_stats.append(sess_stats)
        if ret_arr:
            all_arrays.append(sess_arrays)

    if ret_arr:
        return xrans, all_stats, all_counts, all_arrays
    else:
        return xrans, all_stats, all_counts


#############################################
def trace_stats_by_exp_len_sess(sessions, analyspar, stimpar, n_quants=4, 
                                 qu_idx="all", byroi=True, integ=False, 
                                 ret_arr=False, nan_empty=False, 
                                 baseline=None, datatype="roi"):
    """
    trace_stats_by_exp_len_sess(sessions, analyspar, stimpar)

    Returns trace statistics for the quantiles of interest for each
    session and unexpected length value, for the datatype of interest.

    Required args:
        - sessions (list)      : list of Session objects
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        
    Optional args:
        - n_quants (int)      : number of quantiles to divide sessions into
                                default: 4
        - qu_idx (str or list): indices of quantiles to retain
                                default: "all"
        - byroi (bool)        : If datatype is "roi", if True, returns 
                                statistics for each ROI. If False, returns 
                                statistics across ROIs.
                                default: True
        - integ (bool)        : if True, dF/F is integrated over sequences
                                default: False
        - ret_arr (bool)      : if True, data arrays are returned also
                                default: False
        - nan_empty (bool)    : if a quantile is empty, return NaN arrays 
                                (avoids an error)
                                default: False
        - baseline (num)      : number of seconds to use as baseline. If None,
                                data is not baselined.
                                default: None
        - datatype (str)      : datatype, i.e. ROIs or running
                                default: "roi"

    Returns:
        - xrans (list)            : time values for the 2p frames (None if 
                                     integ), for each session
        - all_stats (list)        : list of 2 to 5D arrays of trace data 
                                    statistics for each session, structured as:
                                        unexp_len x
                                        quantiles x
                                        stats (me, err) x
                                        (ROIs if byroi x)
                                        (frames if not integ)
        - all_counts (nested list) : list of number of sequences, 
                                     structured as:
                                        sess x unexp_len x quantiles
        - all_n_consec (list)      : unique values of number of consecutive 
                                     segments, by session  
        if ret_arr:
        - all_arrays (nested lists): list of data trace arrays, structured as:
                                     session x unexp_len x quantile 
                                     of 1 to 3D arrays: 
                                         (ROI x) sequences 
                                         (x frames if not integ)
    """

    shift_gab_segs = False
    if stimpar.stimtype == "gabors" and stimpar.gabfr not in ["any", "all"]:
        shift_gab_segs = True
        orig_gabfr = stimpar.gabfr
        stimpar = sess_ntuple_util.get_modif_ntuple(stimpar, "gabfr", "any")

    all_counts, all_stats, all_arrays, all_n_consec = [], [], [], []
    xrans = []
    for sess in sessions:
        stim = sess.get_stim(stimpar.stimtype)
        sess_counts, sess_stats, sess_arrays = [], [], []
        qu_segs, _, qu_n_consec = quant_segs(
            stim, stimpar, n_quants, qu_idx, 1, empty_ok=nan_empty, 
            by_exp_len=True)

        if shift_gab_segs: # shift to requested gabor frame
            qu_segs = [[s + orig_gabfr for s in segs] for segs in qu_segs]

        n_consec_flat   = [n for sub_ns in qu_n_consec for n in sub_ns]
        all_n_consec.append(sorted(set(n_consec_flat)))

        for n_consec in all_n_consec[-1]:
            sub_segs, sub_counts = [], []
            # retain segments with correct number of consecutive values
            for segs, ns in zip(qu_segs, qu_n_consec): 
                idx = np.where(np.asarray(ns) == n_consec)[0]
                sub_segs.append([segs[i] for i in idx])
                sub_counts.append(len(idx))
            sess_counts.append(sub_counts)
            trace_info = trace_stats_by_qu(stim, sub_segs, stimpar.pre,
                stimpar.post, analyspar, byroi=byroi, integ=integ, 
                ret_arr=ret_arr, nan_empty=nan_empty, 
                baseline=baseline, datatype=datatype)
            sess_stats.append(trace_info[1])
            if ret_arr:
                sess_arrays.append(trace_info[2])
        xrans.append(trace_info[0])
        all_counts.append(sess_counts)
        all_stats.append(np.asarray(sess_stats))
        if ret_arr:
            all_arrays.append(sess_arrays)

    if ret_arr:
        return xrans, all_stats, all_counts, all_n_consec, all_arrays
    else:
        return xrans, all_stats, all_counts, all_n_consec


#############################################
def run_mag_permute(all_data_perm, act_mag_me_rel, act_L2_rel, n_exps, permpar, 
                    op_qu="diff", op_grp="diff", stats="mean", nanpol=None):
    """
    run_mag_permute(all_data_perm, act_mag_rel, act_L2_rel, n_exp, permpar)

    Returns the results of a permutation analysis of difference or ratio 
    between 2 quantiles of the magnitude change or L2 norm between expected and 
    unexpected activity.

    Required args:
        - all_data_perm (2D array): Data from both groups for permutation, 
                                    structured as:
                                        ROI x seqs
        - act_mag_rel (num)       : Real mean/median magnitude difference
                                    between quantiles
        - act_L2_rel (num)        : Real L2 difference between quantiles
        - n_exps (list)           : List of number of expected sequences in
                                    each quantile
        - permpar (PermPar)       : named tuple containing permutation 
                                    parameters
    
    Optional args:
        - op_qu (str) : Operation to use in comparing the last vs first 
                        quantile ("diff" or "ratio")
                        default: "diff"       
        - op_grp (str): Operation to use in comparing groups 
                        (e.g., unexpected vs expected data) ("diff" or "ratio")
                        default: "diff" 
        - stats (str) : Statistic to take across group sequences, and then 
                        across magnitude differences ("mean" or "median")
                        default: "mean"
        - nanpol (str): Policy for NaNs, "omit" or None when taking statistics
                        default: None
    
    Returns:
        - signif (list) : list of significance results ("hi", "lo" or "no") for 
                          magnitude, L2
        - threshs (list): list of thresholds (1 if 1-tailed analysis, 
                          2 if 2-tailed) for magnitude, L2
    """

    if permpar.multcomp:
        permpar = sess_ntuple_util.get_modif_ntuple(
            permpar, ["multcomp", "p_val"], 
            [False, permpar.p_val / permpar.multcomp]
            )

    if len(all_data_perm) != 2 or len(n_exps) !=2:
        raise ValueError("all_data_perm and n_exps must have length of 2.")

    all_rand_vals = [] # qu x grp x ROI x perms
    # for each quantile
    for q, perm_data in enumerate(all_data_perm):
        qu_vals = rand_util.permute_diff_ratio(
            perm_data, n_exps[q], permpar.n_perms, stats, nanpol=nanpol, 
            op="none")
        all_rand_vals.append(qu_vals)

    all_rand_vals = np.asarray(all_rand_vals)
    # get absolute change stats and retain mean/median only
    rand_mag_me = math_util.calc_mag_change(
        all_rand_vals, 0, 2, order="stats", op=op_qu, stats=stats)[0]
    rand_L2 = math_util.calc_mag_change(all_rand_vals, 0, 2, order=2, op=op_qu)

    # take diff/ratio between grps
    rand_mag_rel = math_util.calc_op(rand_mag_me, op_grp, dim=0)
    rand_L2_rel  = math_util.calc_op(rand_L2, op_grp, dim=0)

    # check significance (returns list although only one result tested)
    mag_sign, mag_th = rand_util.id_elem(
        rand_mag_rel, act_mag_me_rel, permpar.tails, permpar.p_val, ret_th=True)
    L2_sign, L2_th   = rand_util.id_elem(
        rand_L2_rel, act_L2_rel, permpar.tails, permpar.p_val, ret_th=True)

    mag_signif, L2_signif = ["no", "no"]
    if str(permpar.tails) == "2":
        if len(mag_sign[0]) == 1:
            mag_signif = "lo"
        elif len(mag_sign[1]) == 1:
            mag_signif = "hi"
        if len(L2_sign[0]) == 1:
            L2_signif = "lo"
        elif len(L2_sign[1]) == 1:
            L2_signif = "hi"
    elif permpar.tails in ["lo", "hi"]:
        if len(mag_sign) == 1:
            mag_signif = permpar.tails
        if len(L2_sign) == 1:
            L2_signif = permpar.tails

    signif  = [mag_signif, L2_signif]
    threshs = [mag_th[0], L2_th[0]]

    return signif, threshs


#############################################
def qu_mags(all_data, permpar, mouse_ns, lines, stats="mean", error="sem", 
            nanpol=None, op_qu="diff", op_unexp="diff", log_vals=True):
    """
    qu_mags(all_data, permpar, mouse_ns, lines)

    Returns a dictionary containing the results of the magnitudes and L2 
    analysis, as well as the results of the permutation test.

    Specifically, magnitude and L2 norm are calculated as follows: 
        - Magnitude: for unexp and expected segments: 
                         mean/median across ROIs of
                             diff/ratio in average activity between 2 quantiles
        - L2 norm:   for unexp and expected segments: 
                         L2 norm across ROIs of
                             diff/ratio in average activity between 2 quantiles
    
    Significance is assessed based on the diff/ratio between unexpected and 
    expected magnitude/L2 norm results.

    Optionally, the magnitudes and L2 norms are logged for each session, with
    significance indicated.

    Required args:
        - all_data (list)  : nested list of data, structured as:
                                 session x unexp x qu x array[(ROI x) seqs]
        - permpar (PermPar): named tuple containing permutation parameters
        - mouse_ns (list)  : list of mouse numbers (1 per session)
        - lines (list)     : list of mouse lines (1 per session)

    Optional args:
        - stats (str)      : statistic to take across segments, (then ROIs) 
                             ("mean" or "median")
                             default: "mean"
        - error (str)      : statistic to take across segments, (then ROIs) 
                             ("std" or "sem")
                             default: "sem"
        - nanpol (str)     : policy for NaNs, "omit" or None when taking 
                             statistics
                             default: None
        - op_qu (str)      : Operation to use in comparing the last vs first 
                             quantile ("diff" or "ratio")
                             default: "diff"       
        - op_unexp (str)    : Operation to use in comparing the unexpected vs 
                             expected, data ("diff" or "ratio")
                             default: "diff" 
        - log_vals (bool)  : If True, the magnitudes and L2 norms are logged
                             for each session, with significance indicated.

    Returns:
        - mags (dict): dictionary containing magnitude and L2 data to plot.
            ["L2"] (3D array)        : L2 norms, structured as: 
                                           sess x scaled x unexp
            ["mag_st"] (4D array)    : magnitude stats, structured as: 
                                           sess x scaled x unexp x stats
            ["L2_rel_th"] (2D array) : L2 thresholds calculated from 
                                       permutation analysis, structured as:
                                           sess x tail(s)
            ["mag_rel_th"] (2D array): magnitude thresholds calculated from
                                       permutation analysis, structured as:
                                           sess x tail(s)
            ["L2_sig"] (list)        : L2 significance results for each session 
                                       ("hi", "lo" or "no")
            ["mag_sig"] (list)       : magnitude significance results for each 
                                       session 
                                           ("hi", "lo" or "no")
    """


    n_sess = len(all_data)
    n_qu   = len(all_data[0][0])
    scales = [False, True]
    unexps    = ["exp", "unexp"]
    stat_len = 2 + (stats == "median" and error == "std")
    tail_len = 1 + (str(permpar.tails) == "2")

    if n_qu != 2:
        raise ValueError(f"Expected 2 quantiles, but found {n_qu}.")
    if len(unexps) != 2:
        raise ValueError("Expected a length 2 unexpected dim, "
            f"but found length {len(unexps)}.")
    
    mags = {"mag_st": np.empty([n_sess, len(scales), len(unexps), stat_len]),
            "L2"    : np.empty([n_sess, len(scales), len(unexps)])
           }
    
    for lab in ["mag_sig", "L2_sig"]:
        mags[lab] = []
    for lab in ["mag_rel_th", "L2_rel_th"]:
        mags[lab] = np.empty([n_sess, tail_len])

    all_data = copy.deepcopy(all_data)
    for i in range(n_sess):
        logger.info(f"Mouse {mouse_ns[i]}, {lines[i]}:", 
            extra={"spacing": "\n"})
        sess_data_me = []
        # number of expected sequences
        n_exps = [all_data[i][0][q].shape[-1] for q in range(n_qu)]
        for s in range(len(unexps)):
            # take the mean for each quantile
            data_me = np.asarray(
                [math_util.mean_med(all_data[i][s][q], stats, axis=-1, 
                nanpol=nanpol) for q in range(n_qu)])

            if len(data_me.shape) == 1:
                # add dummy ROI-like axis, e.g. for run data
                data_me = data_me[:, np.newaxis]
                all_data[i][s] = \
                    [qu_data[np.newaxis, :] for qu_data in all_data[i][s]]

            msgs = ["Degrees of freedom", "invalid value"]
            categs = [RuntimeWarning, RuntimeWarning]
            with gen_util.TempWarningFilter(msgs, categs):
                mags["mag_st"][i, 0, s] = math_util.calc_mag_change(
                    data_me, 0, 1, order="stats", op=op_qu, stats=stats, 
                    error=error)
                mags["L2"][i, 0, s] = math_util.calc_mag_change(
                    data_me, 0, 1, order=2, op=op_qu)
            sess_data_me.append(data_me)
        # scale
        sess_data_me = np.asarray(sess_data_me)
        mags["mag_st"][i, 1] = math_util.calc_mag_change(
            sess_data_me, 1, 2, order="stats", op=op_qu, stats=stats, 
            error=error, scale=True, axis=0, pos=0, sc_type="unit").T
        mags["L2"][i, 1] = math_util.calc_mag_change(
            sess_data_me, 1, 2, order=2, op=op_qu, stats=stats, scale=True, 
            axis=0, pos=0, sc_type="unit").T
        
        # diff/ratio for permutation test
        act_mag_rel = math_util.calc_op(mags["mag_st"][i, 0, :, 0], op=op_unexp)
        act_L2_rel  = math_util.calc_op(mags["L2"][i, 0, :], op=op_unexp)

        # concatenate expected and unexpected sequences for each quantile
        all_data_perm = [np.concatenate(
            [all_data[i][0][q], all_data[i][1][q]], axis=1) 
                for q in range(n_qu)]
        
        signif, ths = run_mag_permute(
            all_data_perm, act_mag_rel, act_L2_rel, n_exps, permpar, op_qu, 
            op_unexp, stats, nanpol)
        
        mags["mag_sig"].append(signif[0])
        mags["L2_sig"].append(signif[1])
        mags["mag_rel_th"][i] = np.asarray(ths[0])
        mags["L2_rel_th"][i] = np.asarray(ths[1])

        # logs results 
        if log_vals:
            sig_symb = ["", ""]
            for si, sig in enumerate(signif):
                if sig != "no":
                    sig_symb[si] = "*"

            vals = [mags["mag_st"][i, 0, :, 0], mags["L2"][i, 0, :]]
            names = [f"{stats} mag".capitalize(), "L2"]
            for v, (val, name) in enumerate(zip(vals, names)):
                for s, unexp in zip([0, 1], ["(exp) ", "(unexp)"]):
                    logger.info(f"{name} {unexp}: {val[s]:.4f}{sig_symb[v]}", 
                        extra={"spacing": TAB})
        
    return mags


