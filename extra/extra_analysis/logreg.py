"""
logreg.py

This module contains functions to run and analyse logistic regressions 
to predict stimulus information from ROI activity for data generated by the 
Allen Institute OpenScope experiments for the Credit Assignment Project.

Authors: Colleen Gillon

Date: October, 2018

Note: this code uses python 3.7.

"""

import copy
import warnings
from pathlib import Path

import numpy as np
import pandas as pd

from extra_analysis import quant_analys
from util import data_util, file_util, gen_util, logger_util, logreg_util, \
    math_util, plot_util, rand_util
from sess_util import sess_gen_util, sess_ntuple_util, sess_str_util
from extra_plot_fcts import logreg_plots

from util import gen_util


TAB = "    "


#### ALWAYS SET TO FALSE - CHANGE ONLY FOR TESTING PURPOSES
TEST_VISFLOW_VARIATIONS = False


logger = logger_util.get_module_logger(name=__name__)


#############################################
def get_comps(stimtype="gabors", q1v4=False, exp_v_unexp=False):
    """
    get_comps()

    Returns comparisons that fit the criteria.

    Optional args:
        - stimtype (str) : stimtype
                           default: "gabors"
        - q1v4 (bool)    : if True, analysis is trained on first and tested on 
                           last quartiles
                           default: False
        - exp_v_unexp (bool): if True, analysis is trained on expected and tested 
                           on unexpected sequences
                           default: False
    
    Returns:
        - comps (list): list of comparisons that fit the criteria
    """

    if stimtype == "gabors":
        if exp_v_unexp:
            raise ValueError("exp_v_unexp can only be used with visual flow.")
        comps = ["unexp", "AvB", "AvC", "BvC", "DvU", "Aori", "Bori", "Cori", 
            "Dori", "Uori", "DoriU", "DoriA", "BCDoriA", "BCDoriU", "ABCoriD", 
            "ABCoriU"]
    elif stimtype == "visflow":
        comps = ["unexp", "dir_all", "dir_unexp", "dir_exp", "half_right", 
            "half_left", "half_diff"] 
        if exp_v_unexp:
            comps = gen_util.remove_if(
                comps, ["unexp", "dir_unexp", "dir_all", "half_right", 
                "half_left", "half_diff"])
        if q1v4:
            comps = gen_util.remove_if(
                comps, ["half_left", "half_right", "half_diff"])
    else:
        gen_util.accepted_values_error(
            "stimtype", stimtype, ["gabors", "visflow"])

    return comps


#############################################
def get_class_pars(comp="unexp", stimtype="gabors"):
    """
    get_class_pars()
    
    Returns name of the class determining variable, and the unexpected values to 
    use for the classes.

    Optional args:
        - comp (str)            : type of comparison
                                  default: "unexp"
        - stimtype (str)        : stimulus type
                                  default: "gabors"

    Returns:
        - class_var (str)    : variable separating classes (e.g., "unexps", 
                              "gab_ori", "visflow_dir")
        - unexps (str or list): unexpected values (for each class, if list)
    """

    if stimtype == "gabors":
        if comp == "unexp":
            class_var = "unexps"
            unexps = [0, 1]
        elif comp == "DvU":
            class_var = "unexps"
            unexps = [0, 1]
        elif "ori" in comp:
            class_var = "gab_ori"
            gab_letts = [lett.upper() for lett in comp.split("ori") 
                if len(lett) > 0]
            unexps = []
            for lett in gab_letts:
                if ("D" in lett) ^ ("U" in lett): # exclusive or
                    unexp_val = 1 if "U" in lett else 0
                    unexps.append(unexp_val)
                else:
                    unexps.append("any")
            if len(gab_letts) == 1:
                unexps = unexps[0]
            
        elif "dir" in comp:
            raise ValueError("dir comparison not valid for gabors.")
        else:
            class_var = "gabfr"
            unexps = "any"

    elif stimtype == "visflow":
        class_var = "visflow_dir"
        if comp == "dir_all":
            unexps = "any"
        elif comp == "dir_exp":
            unexps = 0
        elif comp == "dir_unexp":
            unexps = 1
        elif comp == "unexp":
            unexps = [0, 1]
            class_var = "unexps"
        elif comp in ["half_right", "half_left", "half_diff"]:
            unexps = "any"
            class_var = comp
        else:
            raise ValueError("Only unexp, dir_all, dir_exp, dir_unexp, "
                "samehalf, diffhalf comparisons supported for Visflow.")

    return class_var, unexps


#############################################
def get_stimpar(comp="unexp", stimtype="gabors", visflow_dir="both", 
                visflow_size=128, gabfr=0, gabk=16, gab_ori="all", 
                visflow_pre=0.0):
    """
    get_stimpar()
    
    Returns a stimulus parameter named tuple based on the stimulus parameters 
    passed and comparison type.

    Optional args:
        - comp (str)                : type of comparison
                                      default: "unexp"
        - stimtype (str)            : stimulus type
                                      default: "gabors"
        - visflow_dir (str or list) : visual flow direction
                                      default: "both"
        - visflow_size (int or list): visual flow direction
                                      default: 128
        - gabfr (int or list)       : gabor frame of reference (may be a list 
                                      depending on "comp")
                                      default: 0
        - gabk (int or list)        : gabor kappa
                                      default: 16
        - gab_ori (str or list)     : gabor orientations (e.g., "all"), 
                                      for comp values like DoriU, DoriA, etc.
                                      default: "all"
        - visflow_pre (int)         : pre parameter for Visflow
                                      default: 0.0 

    Returns:
        - stimpar (StimPar)  : named tuple containing stimulus parameters
    """

    if stimtype == "visflow" and "half" in visflow_dir or "dir" in visflow_dir:
        logger.info("Ignoring visual flow direction setting.")

    [visflow_dir, visflow_size, gabfr, 
        gabk, gab_ori] = sess_gen_util.get_params(
            stimtype, visflow_dir, visflow_size, gabfr, gabk, gab_ori)

    if stimtype == "gabors":
        # DO NOT ALLOW OVERLAPPING
        if comp == "unexp":
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, visflow_dir,  visflow_size, gabfr, gabk, gab_ori, 
                0, 1.5)
        elif comp == "DvU":
            gabfr   = sess_str_util.gabfr_nbrs(comp[0])
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, visflow_dir, visflow_size, gabfr, gabk, gab_ori, 
                0, 0.45)
        elif "ori" in comp:
            gab_letts = [lett.upper() for lett in comp.split("ori")
                if len(lett) > 0]
            act_gabfr = [[sess_str_util.gabfr_nbrs(lett) for lett in letts] 
                for letts in gab_letts]
            if len(act_gabfr) == 1:
                pre, post = 0, 0.45
                if comp in ["Dori", "Uori"]:
                    pre, post = 0, 0.6
                act_gabfr = act_gabfr[0][0] # only one value
                gab_ori = sess_gen_util.filter_gab_oris(gab_letts[0], gab_ori)
                if act_gabfr != gabfr:
                    logger.info(
                        f"Setting gabfr to {act_gabfr} instead of {gabfr}.")
            else:
                pre, post = -0.15, 0.45
                gab_ori = sess_gen_util.gab_oris_common_U(gab_ori)
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, visflow_dir, visflow_size, act_gabfr, gabk, gab_ori, 
                pre, post)
        elif "dir" in comp or "half" in comp:
            raise ValueError("dir/half comparison not valid for gabors.")
        else:
            gabfrs = sess_str_util.gabfr_nbrs([comp[0], comp[2]])
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, visflow_dir, visflow_size, gabfrs, gabk, gab_ori, 
                0, 0.45)
    elif stimtype == "visflow":
        # DO NOT ALLOW OVERLAPPING
        if "right" in comp:
            visflow_dir = "right"
        elif "left" in comp:
            visflow_dir = "left"
        stimpar = sess_ntuple_util.init_stimpar(
            stimtype, visflow_dir, visflow_size, gabfr, gabk, gab_ori, 
            visflow_pre, 1.0)

        # for visual flow logreg test analyses
        if TEST_VISFLOW_VARIATIONS:
            logger.warning("Setting visual flow pre/post to 2 for testing purposes.")
            stimpar = sess_ntuple_util.init_stimpar(
                stimtype, visflow_dir, visflow_size, gabfr, gabk, gab_ori, 2, 2)

    return stimpar


#############################################
def get_rundir(run_val, uniqueid=None, alg="sklearn"):
    """
    get_rundir(run_val)

    Returns the name of the specific subdirectory in which an analysis is
    saved, based on a run number and unique ID.
    
    Required args:
        - run_val (int): run number ("pytorch" alg) or 
                         number of run ("sklearn" alg)
    
    Optional args:
        - uniqueid (str or int): unique ID for analysis
                                 default: None
        - alg (str)            : algorithm used to run logistic regression 
                                 ("sklearn" or "pytorch")
                                 default: "sklearn"

    Returns:
        - rundir (Path): name of subdirectory to save analysis in
    """


    if uniqueid is None:
        if alg == "sklearn":
            rundir = f"{run_val}_runs"
        elif alg == "pytorch":
            rundir = f"run_{run_val}"
        else:
            gen_util.accepted_values_error("alg", alg, ["sklearn", "pytorch"])
    else:
        rundir = f"{uniqueid}_{run_val}"
    rundir = Path(rundir)

    return rundir


#############################################
def get_compdir_dict(rundir, no_lists=False):
    """
    get_compdir_dict(rundir)

    Returns a dictionary with analysis parameters based on the full analysis 
    path.
    
    Required args:
        - rundir (Path): path of subdirectory in which analysis is saved,
                         structured as 
                         ".../m_s_plane_stim_fluor_scaled_comp_shuffled/
                         uniqueid_run"
    
    Optional args:
        - no_lists (bool): if True, list parameters are replaced with a string, 
                           e.g. "both"
                           False
    
    Returns:
        - compdir_dict (dict): parameter dictionary
            - visflow_dir (str or list): visual flow direction parameter 
                                         ("right", "left", ["right", "left"] 
                                         or "none") 
            - visflow_size (int or list): visual flow size parameter (128, 256, 
                                          [128, 256] or "none")
            - comp (str)                : comparison parameter ("unexp", "AvB",
                                          "AvC", "BvC" or "DvU", None)
            - fluor (str)               : fluorescence parameter ("raw" or "dff")
            - gabk (int or list)        : Gabor kappa parameter 
                                          (4, 16, [4, 16] or "none")
            - plane (str)               : plane ("soma" or "dend")
            - mouse_n (int)             : mouse number
            - sess_n (int)              : session number
            - scale (bool)              : scaling parameter
            - run_n (int)               : run number
            - shuffle (bool)            : shuffle parameter
            - stimtype (str)            : stimulus type ("gabors" or "visflow")
            - uniqueid (str)            : unique ID (datetime, 6 digit number or 
                                          None)
    """

    param_str = rundir.parts[-2]
    run_str   = rundir.parts[-1]

    compdir_dict = sess_gen_util.get_params_from_str(param_str, no_lists)

    if "run" in run_str:
        compdir_dict["uniqueid"] = None
        compdir_dict["run_n"]    = int(run_str.split("_")[1])
    else:
        compdir_dict["uniqueid"] = "_".join(
            [str(sub) for sub in run_str.split("_")[:-1]])
        compdir_dict["run_n"]    = int(run_str.split("_")[-1])    

    return compdir_dict


#############################################
def get_df_name(task="analyse", stimtype="gabors", comp="unexp", ctrl=False, 
                alg="sklearn"):
    """
    get_df_name()

    Returns a dictionary with analysis parameters based on the full analysis 
    path.
    
    Optional args:
        - task (str)    : type of task for which to get the dataframe 
                          default: "analyse"
        - stimtype (str): type of stimulus
                          default: "gabors"
        - comp (str)    : type of comparison
                          default: "unexp"
        - ctrl (bool)   : if True, control comparisons are analysed
                          default: False
        - alg (str)     : algorithm used to run logistic regression 
                          ("sklearn" or "pytorch")
                          default: "sklearn"

    Returns:
        - df_name (str): name of the dataframe
    """

    alg_str = ""
    if alg == "pytorch":
        alg_str = "_pt"
    elif alg != "sklearn":
        gen_util.accepted_values_error("alg", alg, ["pytorch", "sklearn"])

    ctrl_str = sess_str_util.ctrl_par_str(ctrl)

    stim_str = "gab" if stimtype == "gabors" else stimtype
    sub_str = f"{stim_str}_{comp}{ctrl_str}{alg_str}"

    if task == "collate":
        df_name = f"{sub_str}_all_scores_df.csv"
    elif task == "analyse":
        df_name = f"{sub_str}_score_stats_df.csv"
    
    return df_name


#############################################
def info_dict(analyspar=None, sesspar=None, stimpar=None, extrapar=None, 
              comp="unexp", alg="sklearn", n_rois=None, epoch_n=None):
    """
    info_dict()

    Returns an info dictionary from the parameters. Includes epoch number if it 
    is passed. 

    Returns an ordered list of keys instead if any of the dictionaries or
    namedtuples are None.
    
    Required args:
        - analyspar (AnalysPar): named tuple containing analysis parameters
                                 default: None
        - sesspar (SessPar)    : named tuple containing session parameters
                                 default: None
        - stimpar (StimPar)    : named tuple containing stimulus parameters
                                 default: None
        - extrapar (dict)      : dictionary with extra parameters
                                 default: None
            ["run_n"] (int)   : run number
            ["shuffle"] (bool): whether data is shuffled
            ["uniqueid"] (str): uniqueid

    Optional args:
        - comp (str)   : comparison type
                         default: "unexp"
        - alg (str)    : algorithm used to run logistic regression 
                         ("sklearn" or "pytorch")
                         default: "sklearn"
        - n_rois (int) : number of ROIs
                         default: None
        - epoch_n (int): epoch number
                         default: None
    
    Returns:
        if all namedtuples and dictionaries are passed:
            - info (dict): analysis dictionary
        else if any are None:
            - info (list): list of dictionary keys
    """

    if not any(par is None for par in [analyspar, sesspar, stimpar, extrapar]):
        if stimpar.stimtype == "visflow":
            visflow_dir = gen_util.list_if_not(stimpar.visflow_dir)
            if len(visflow_dir) == 2:
                visflow_dir = "both"
            else:
                visflow_dir = visflow_dir[0]
        else:
            visflow_dir = stimpar.visflow_dir

        info = {"mouse_n"    : sesspar.mouse_n,
                "sess_n"     : sesspar.sess_n,
                "plane"      : sesspar.plane,
                "line"       : sesspar.line,
                "fluor"      : analyspar.fluor,
                "scale"      : analyspar.scale,
                "shuffle"    : extrapar["shuffle"],
                "stimtype"   : stimpar.stimtype,
                "visflow_dir": visflow_dir,
                "comp"       : comp,
                "uniqueid"   : extrapar["uniqueid"],
                "runtype"    : sesspar.runtype,
                "n_rois"     : n_rois
                }

        if alg == "pytorch":
            info["run_n"] = extrapar["run_n"]

        if epoch_n is not None:
            info["epoch_n"] = epoch_n

    # if no args are passed, just returns keys
    else:
        info = ["mouse_n", "sess_n", "plane", "line", "fluor", "scale", 
            "shuffle", "stimtype", "visflow_dir", "comp", "uniqueid", "run_n", 
            "runtype", "n_rois", "epoch_n"]
    return info


#############################################
def save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar): 
    """
    save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    Saves the hyperparameters for an analysis.
    
    Required args:
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ["dirname"] (str): directory in which to save hyperparameters
    
    Returns:
        - hyperpars (dict): hyperparameter dictionary with inputs as keys and 
                            named tuples converted to dictionaries
    """

    hyperpars = {"analyspar": analyspar._asdict(),
                 "logregpar": logregpar._asdict(),
                 "sesspar"  : sesspar._asdict(),
                 "stimpar"  : stimpar._asdict(),
                 "extrapar" : extrapar
                }

    file_util.saveinfo(hyperpars, "hyperparameters.json", extrapar["dirname"])

    return hyperpars


#############################################
def get_classes(comp="unexp", gab_ori="all"):
    """
    get_classes()

    Returns names for classes based on the comparison type.
    
    Optional args:
        - comp (str)           : type of comparison
                                 default: "unexp"
        - gab_ori (str or list): Gabor orientations
                                 default: "all"
    Returns:
        - classes (list): list of class names
    """

    if comp == "unexp":
        classes = ["Expected", "Unexpected"]
    elif comp in ["AvB", "AvC", "BvC", "DvU"]:
        classes = [f"Gabor {fr}" for fr in [comp[0], comp[2]]]    
    elif "ori" in comp:
        deg_vals = gab_ori
        stripped = comp.replace("ori", "")
        gab_ori = sess_gen_util.filter_gab_oris(stripped, gab_ori)
        deg = u"\u00B0"
        classes = [f"{val}{deg}" for val in deg_vals]
    elif "dir" in comp:
        classes = [sess_str_util.dir_par_str(
            direc, str_type="print").replace(
                "visflow (", "").replace(", ", " (").capitalize() 
            for direc in ["right", "left"]]
    elif "half" in comp:
        classes = ["First half", "Second half"]
    else:
        gen_util.accepted_values_error("comp", comp, 
            ["unexp", "AvB", "AvC", "BvC", "DvU", "dir...", "...ori..."])

    return classes


#############################################
def get_data(stim, analyspar, stimpar, quantpar, qu_i=0, unexp=[0, 1], 
             n=1, remconsec_unexps=False, get_2nd=False):
    """
    get_data(sess, quantpar, stimpar)

    Returns ROI data based on specified criteria. 

    Required args:
        - stim (Stim)          : stimulus object
        - analyspar (AnalysPar): named tuple containing analysis parameters        
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - quantpar (QuantPar)  : named tuple containing quantile parameters
    
    Optional args:
        - qu_i (int)            : quartile index
                                  default: 0
        - unexp (list)           : unexpected values
                                  default: [0, 1]
        - n (int)               : factor by which to multiply number of 
                                  unexpected values
                                  default: 1
        - remconsec_unexps (bool): whether consecutive segments are removed for 
                                  unexpected segments
                                  default: False
        - get_2nd (bool)        : if True, every second segment is retained
                                  default: False

    Returns:
        - roi_data (3D array): ROI data, as sequences x frames x ROIs
        - unexp_n (int)       : Number of unexpected sequences
    """
   
    # data for single quartile
    # first number of unexpecteds, then segs
    for t, unexp_use in enumerate([1, unexp]):
        # adjust orientation if U frames are used as a control
        stimpar_use = stimpar
        if (stimpar.gabfr in [3, 4] and isinstance(stimpar.gab_ori, int) and 
            unexp_use == 1 and unexp == 0):
            gab_ori = sess_gen_util.get_unexp_gab_ori(stimpar.gab_ori)
            stimpar_use = sess_ntuple_util.get_modif_ntuple(
                stimpar, "gab_ori", gab_ori
                )

        remconsec = (remconsec_unexps and unexp_use == 1)
        segs = quant_analys.quant_segs(
            stim, stimpar_use, quantpar.n_quants, qu_i, unexp_use, 
            remconsec=remconsec)[0][0]

        # get alternating for consecutive segments
        if get_2nd and not remconsec: 
            segs = gen_util.get_alternating_consec(segs, first=False)
        if t == 0:
            unexp_n = len(segs) * n
    
    twop_fr = stim.get_fr_by_seg(
        segs, start=True, fr_type="twop"
        )["start_frame_twop"]

    # do not scale (scaling factors cannot be based on test data)
    if stim.sess.only_tracked_rois != analyspar.tracked:
        raise RuntimeError(
            "stim.sess.only_tracked_rois should match analyspar.tracked."
            )
    roi_data = gen_util.reshape_df_data(
        stim.get_roi_data(twop_fr, stimpar.pre, stimpar.post, 
        analyspar.fluor, rem_bad=True, scale=False), squeeze_cols=True)
    
    # for visual flow logreg test analyses
    if stimpar.stimtype == "visflow" and TEST_VISFLOW_VARIATIONS:
        if remconsec_unexps:
            # Normalize to first half
            mid = roi_data.shape[-1]  // 2
            div = np.median(roi_data[:, :, : mid], axis=-1)
            roi_data = roi_data - np.expand_dims(div, -1)
            
            # # Mean only
            if TEST_VISFLOW_VARIATIONS == "mean":
                logger.warning("Using mean across ROIs, for testing purposes.")
                # 1 x seqs x frames
                roi_data = np.expand_dims(np.nanmean(roi_data, axis=0), axis=0)

            # Mean and std
            elif TEST_VISFLOW_VARIATIONS == "mean_std":
                logger.warning("Using mean and standard deviation across ROIs, "
                    "for testing purposes.")
                roi_data = np.stack([np.nanmean(roi_data, axis=0),
                    np.nanstd(roi_data, axis=0)], axis=0)
        

    # transpose to seqs x frames x ROIs
    roi_data = np.transpose(roi_data, [1, 2, 0])

    return roi_data, unexp_n


#############################################
def get_sess_data(sess, analyspar, stimpar, quantpar, class_var="unexps", 
                  unexps=[0, 1], exp_v_unexp=False, split_oris=False):
    """
    get_sess_data(sess, analyspar, stimpar, quantpar)

    Logs session information and returns ROI trace segments, target classes 
    and class information and number of unexpected segments in the dataset.
    
    Required args:
        - sess (Session)       : session
        - analyspar (AnalysPar): named tuple containing analysis parameters        
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - quantpar (QuantPar)  : named tuple containing quantile parameters

    Optional args:
        - class_var (str)          : class determining variable ("unexps" or 
                                     stimpar attribute)
                                     default: "unexps"
        - unexps (list, str, int)   : unexpected value(s) (list if class_var is 
                                     "unexps", otherwise 0, 1 or "any")
        - exp_v_unexp (bool)       : if True, the first dataset will include 
                                     expected sequences and the second will 
                                     include unexpected sequences
                                     default: False
        - split_oris (bool or list): List of Gabor frames for each split, or 
                                     False if splitting orientation comparison 
                                     is not applicable.
                                     default: False
    Returns:
        - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs
                              (1 or 2 if an additional test set is included), 
                              each structured as sequences x frames x ROIs
        - seq_classes (list): list of 2D arrays of sequence classes
                              (1 or 2 if an additional test set is included), 
                              each structured as class values x 1
        - n_unexps (list)    : list of lists of number of unexpected sequences
                              (doubled if "half" comparison), 
                              structured as datasets x class 
    """

    stim = sess.get_stim(stimpar.stimtype)

    split_oris = split_oris is not False # set to boolean

    if (exp_v_unexp + (len(quantpar.qu_idx) > 1) + ("half" in class_var)
        + split_oris) > 1:
        raise ValueError("Cannot combine any of the following: separating "
            "quartiles, exp_v_unexp, half comparisons, multiple Gabor frame "
            "orientation comparisons.")
    elif len(quantpar.qu_idx) > 2:
        raise ValueError("Max of 2 quartiles expected.")
    elif split_oris and len(stimpar.gabfr) > 2:
        raise ValueError("Max of 2 Gabor frame sets expected for orientation "
            "classification.")

    # check for stimulus pre/post problems
    pre_post_err = False
    get_2nd, remconsec_unexps = False, False
    if stimpar.pre > 0:
        if stimpar.stimtype == "visflow":
            if class_var == "unexps":
                remconsec_unexps = True
            elif stimpar.pre == 1:
                get_2nd = True
            else:
                pre_post_err = True
        else:
            pre_post_err = True
    if stimpar.post > 1.0:
        if not stimpar.stimtype == "gabors" and stimpar.post <= 1.5:
            pre_post_err = True
    if pre_post_err:
        raise NotImplementedError("Not implemented to prevent sequence overlap "
            f"for {stimpar.stimtype}: {stimpar.pre} pre/{stimpar.post} post "
            f"for {class_var} classification")

    n = 1
    if class_var == "unexps":
        n_cl = len(unexps)
    elif "half" in class_var:
        n_cl = 2
        # DOUBLE unexp ns to compensate for shorter blocks, if using control
        n = 2
        if "diff" in class_var:
            quantpar = sess_ntuple_util.init_quantpar(4, [[1, 2]])
            if len(np.unique(stim.main_flow_direcs)) != 2:
                raise RuntimeError(
                    "Segments do not fit these criteria (missing directions).")
        else:
            quantpar = sess_ntuple_util.init_quantpar(2, [[0, 1]])
    else:
        n_cl = len(stimpar._asdict()[class_var])

    # modify unexps, qu_idx, gabfr to cycle through datasets
    if len(quantpar.qu_idx) == 2:
        unexps = [unexps, unexps]
        gabfr_idxs = ["ignore", "ignore"]
        if exp_v_unexp:
            raise ValueError(
                "Cannot set exp_v_unexp to True if more than 1 quantile.")
        if "part" in class_var:
            raise ValueError("Cannot do half comparisons with quartiles.")
    elif exp_v_unexp:
        unexps = [unexps, 1-unexps]
        gabfr_idxs = ["ignore", "ignore"]
        quantpar = sess_ntuple_util.init_quantpar(1, [0, 0])
    elif split_oris:
        unexps = unexps
        gabfr_idxs = [0, 1]
        quantpar = sess_ntuple_util.init_quantpar(1, [0, 0])
    else:
        unexps = [unexps]
        gabfr_idxs = ["ignore"]
    gabfr_idxs = [0, 1] if split_oris else ["ignore", "ignore"]

    # cycle through classes
    roi_seqs    = [[] for _ in range(len(quantpar.qu_idx))]
    seq_classes = [[] for _ in range(len(quantpar.qu_idx))]
    unexp_ns     = [[] for _ in range(len(quantpar.qu_idx))]

    # cycle through data groups (quant or exp_v_unexp or gabfr for oris)        
    for d, (qu_i, sub_unexps, gabfr_idx) in enumerate(
        zip(quantpar.qu_idx, unexps, gabfr_idxs)):
        for cl in range(n_cl):
            use_qu_i = [qu_i]
            unexp = sub_unexps
            stimpar_sp = stimpar
            if class_var == "unexps":
                unexp = sub_unexps[cl]
            elif "half" in class_var:
                use_qu_i = [qu_i[cl]]
            else:
                keys = class_var
                vals = stimpar._asdict()[class_var][cl]
                if split_oris:
                    keys = [keys, "gabfr", "gab_ori"]
                    gabfr = stimpar.gabfr[gabfr_idx]
                    gab_ori = stimpar.gab_ori[cl]
                    vals = [vals, gabfr, gab_ori]
                # modify stimpar
                stimpar_sp = sess_ntuple_util.get_modif_ntuple(
                    stimpar, keys, vals)

            roi_data, unexp_n = get_data(
                stim, analyspar, stimpar_sp, quantpar, qu_i=use_qu_i, 
                unexp=unexp, remconsec_unexps=remconsec_unexps, n=n,  
                get_2nd=get_2nd)

            roi_seqs[d].append(roi_data)
            seq_classes[d].append(np.full(len(roi_data), cl))
            unexp_ns[d].append(unexp_n)

        # concatenate data split by class along trial seqs axis
        roi_seqs[d] = np.concatenate(roi_seqs[d], axis=0)
        seq_classes[d] = np.concatenate(seq_classes[d], axis=0)
        
    # get logistic variance across datasets
    log_var = np.log(np.var(np.concatenate(roi_seqs, axis=0)))
    n_fr, nrois = roi_seqs[0].shape[1:] # in training set

    if stimpar.stimtype == "gabors":
        unexp_use = unexps[0]
        if unexp_use == [0, 1] and not isinstance(stimpar.gabfr, list):
            unexp_use = "any"
        if split_oris:
            gabfr_lett = [sess_str_util.gabfr_letters(
                gabfr, unexp=unexp_use) for gabfr in stimpar.gabfr]
            gabfr_lett = " -> ".join([str(lett) for lett in gabfr_lett])
        else:
            gabfr_lett = sess_str_util.gabfr_letters(
                stimpar.gabfr, unexp=unexp_use)
        stim_info = f"\nGab fr: {gabfr_lett}\nGab K: {stimpar.gabk}"
    elif stimpar.stimtype == "visflow":
        stim_info = (f"\nVisual flow dir: {stimpar.visflow_dir}\n"
            f"Visual flow size: {stimpar.visflow_size}")

    logger.info(f"Runtype: {sess.runtype}\nMouse: {sess.mouse_n}\n"
        f"Sess: {sess.sess_n}\nPlane: {sess.plane}\nLine: {sess.line}\n"
        f"Fluor: {analyspar.fluor}\nROIs: {nrois}{stim_info}\n"
        f"Frames per seg: {n_fr}\nLogvar: {log_var:.2f}", 
        extra={"spacing": "\n"})

    return roi_seqs, seq_classes, unexp_ns


#############################################
def sample_seqs(roi_seqs, seq_classes, n_unexp):
    """
    sample_seqs(roi_seqs, seq_classes, n_unexp)

    Samples sequences to correspond to the ratio of unexpected to expected 
    sequences.
    
    Required args:
        - roi_seqs (3D array)   : array of all ROI trace sequences, structured 
                                  as: sequences x frames x ROIs
        - seq_classes (2D array): array of all sequence classes (0, 1), 
                                  structured as class values x 1
        - n_unexp (int)          : number of unexpected sequences

    Returns:
        - roi_seqs (3D array)   : array of selected ROI trace sequences, 
                                  structured as sequences x frames x ROIs
        - seq_classes (2D array): array of sequence classes, structured as 
                                  class values x 1
    """
    if np.unique(seq_classes).tolist() != [0, 1]:
        raise ValueError("Function expects classes 0 and 1 only.")

    class0_all = np.where(seq_classes == 0)[0]
    class1_all = np.where(seq_classes == 1)[0]
    n_exp = (len(class0_all) + len(class1_all))//2 - n_unexp

    class0_idx = np.random.choice(class0_all, n_exp, replace=False)
    class1_idx = np.random.choice(class1_all, n_unexp, replace=False)
    
    roi_seqs = np.concatenate(
        [roi_seqs[class0_idx], roi_seqs[class1_idx]], axis=0)

    seq_classes = np.concatenate(
        [seq_classes[class0_idx], seq_classes[class1_idx]], axis=0)
    return roi_seqs, seq_classes


#############################################
def save_tr_stats(plot_data, plot_targ, data_names, analyspar, stimpar, n_rois, 
                  alg="sklearn",mod=None, dirname="."):
    """
    save_tr_stats(plot_data, plot_targ, data_names, stimpar, n_rois)

    Extracts, saves and returns trace statistics in a json in the specified 
    directory.

    Required args:
        - plot_data (list)     : list of 3D arrays of selected ROI trace seqs 
                                 to be plotted, each structured as 
                                      sequences x frames x ROIs
        - plot_targ (list)     : list of 2D arrays of sequence classes to be 
                                 plotted, each structured as class values x 1
        - data_names (list)    : names for each plot_data array
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (SessPar)    : named tuple containing stimulus parameters
        - n_rois (int)         : number of ROIs in data
    
    Optional args:
        - alg (str)             : algorithm used to run logistic regression 
                                  ("sklearn" or "pytorch")
                                  default: "sklearn"
        - mod (sklearn pipeline): sklearn pipeline (model). Required if alg is 
                                  "sklearn"
                                  default: None
        - dirname (Path)        : directory in which to save the traces
                                  default: "."

    Returns:
        - tr_stats (dict)           : dictionary of trace stats data
            ["n_rois"] (int)                  : number of ROIs
            ["train_ns"] (list)               : number of segments per class
            ["train_class_stats"] (3D array)  : training statistics, structured
                                                as class x stats (me, err) x 
                                                   frames
            ["xran"] (array-like)             : x values for frames
            
            optionally, if an additional named set (e.g., "test_Q4") is passed:
            ["set_ns"] (list)             : number of segments per class
            ["set_class_stats"] (3D array): trace statistics, 
                                                  structured as 
                                                  class x stats (me, err) x 
                                                  frames
    """

    if len(data_names) != len(plot_data):
        raise ValueError("Expected as many 'plot_data' items as 'data_names'.")

    tr_stats = {"n_rois": n_rois}
    classes = np.unique(plot_targ[0])

    for data, targ, name in zip(plot_data, plot_targ, data_names): # get stats
        if data is None:
            continue
        if alg == "sklearn":
            # scales, flattens and optionally shuffles data
            data = logreg_util.get_transf_data_sk(mod, data, False, 
                name=="train")
        elif alg == "pytorch":
            data = data.numpy()
            targ = targ.numpy()
        else:
            gen_util.accepted_values_error("alg", alg, ["sklearn", "pytorch"])
        xran, class_stats, ns = logreg_util.get_stats(
            data, targ, stimpar.pre, stimpar.post, classes, analyspar.stats, 
            analyspar.error)
        tr_stats["xran"] = xran.tolist()
        tr_stats[f"{name}_class_stats"] = class_stats.tolist()
        tr_stats[f"{name}_ns"] = ns

    file_util.saveinfo(tr_stats, "tr_stats.json", dirname)

    return tr_stats


#############################################
@logreg_util.catch_set_problem
def init_logreg_model_pt(roi_seqs, seq_classes, logregpar, extrapar, 
                         scale=True, device="cpu", thresh_cl=2):
    """
    init_logreg_model_pt(roi_seqs, seq_classes, logregpar, extrapar)

    Initializes and returns the pytorch logreg model and dataloaders.
    
    Required args:
        - roi_seqs (list)       : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
        - seq_classes (list)    : list of 2D arrays of sequence classes (0 or 1)
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
        - logregpar (LogRegPar) : named tuple containing logistic regression 
                                  parameters
        - extrapar (dict)       : dictionary with extra parameters
            ["shuffle"] (bool): if True, data is shuffled
    
    Optional args:
        - scale (bool)          : whether data is scaled by ROI
                                  default: True
        - device (str)          : device to use
                                  default: "cpu"
        - thresh_cl (int)       : size threshold for classes in each non empty 
                                  set beneath which the indices are reselected 
                                  (only if targets are passed). Not checked if 
                                  thresh_cl is 0.
                                  default: 2

    Returns:
        - model (torch.nn.Module)        : Neural network module with optimizer 
                                           and loss function as attributes
        - dls (list of torch DataLoaders): list of torch DataLoaders for 
                                           each set. If a set is empty, the 
                                           corresponding dls value is None.
        - extrapar (dict)                : dictionary with extra parameters
            ["cl_wei"] (list)      : list of weights for each class
            ["loss_name"] (str)    : name of the loss function used
            ["sc_facts"] (list)    : min/max value(s) with which to scale
            ["shuffle"] (bool)     : if True, data is shuffled
            ["shuff_reidx"] (list) : list of indices with which targets were 
                                     shuffled 
    """

    # avoid torch dependency for the whole module
    import torch
    from util import torch_data_util 

    sc_dim = "last" if scale else "none"

    if np.unique(seq_classes[0]).tolist() != [0, 1]:
        raise NotImplementedError("This Pytorch logreg function is "
            "implemented only for classes 0 and 1.")

    if len(roi_seqs) > 2:
        raise ValueError("Must pass no more than 2 sets of data, but "
            f"found {len(roi_seqs)}.")

    dl_info = torch_data_util.create_dls(
        roi_seqs[0], seq_classes[0], train_p=logregpar.train_p, sc_dim=sc_dim, 
        sc_type="stand_rob", extrem="perc", shuffle=extrapar["shuffle"], 
        batchsize=logregpar.batchsize, thresh_cl=thresh_cl)
    dls = dl_info[0]

    extrapar = copy.deepcopy(extrapar)

    if scale:
       extrapar["sc_facts"] = dl_info[-1]
    
    if len(roi_seqs) == 2:
        class_vals = seq_classes[1]
        if extrapar["shuffle"]:
            np.random.shuffle(class_vals)
        if scale:
            roi_seqs[1] = torch_data_util.scale_datasets(
                torch.Tensor(roi_seqs[1]), sc_facts=extrapar["sc_facts"])[0]
        dls.append(torch_data_util.init_dl(
            roi_seqs[1], class_vals, logregpar.batchsize))

    if extrapar["shuffle"]:
        extrapar["shuff_reidx"] = dl_info[1]
 
    # from train targets
    extrapar["cl_wei"] = logreg_util.class_weights(dls[0].dataset.targets) 

    n_fr, n_rois  = roi_seqs[0].shape[1:]
    model         = logreg_util.LogReg(n_rois, n_fr).to(device)
    model.opt     = torch.optim.Adam(
        model.parameters(), lr=logregpar.lr, weight_decay=logregpar.wd)
    model.loss_fn = logreg_util.weighted_BCE(extrapar["cl_wei"])
    extrapar["loss_name"] = model.loss_fn.name
    
    return model, dls, extrapar


#############################################
def save_scores(info, scores, key_order=None, dirname="."):
    """
    save_scores(args, scores, saved_eps)

    Saves run information and scores per epoch as a dataframe.
    
    Required args:
        - info (dict)          : dictionary of parameters (see info_dict())
        - scores (pd DataFrame): dataframe of recorded scores
    
    Optional args:
        - key_order (list): ordered list of keys
                            default: None
        - dirname (Path)  : directory in which to save scores
                            default: "."
    
    Returns:
        - summ_df (pd DataFrame): dataframe with scores and recorded parameters
    """

    summ_df = copy.deepcopy(scores)

    if key_order is None:
        key_order = info.keys()

    for key in reversed(key_order):
        if key in info.keys():
            summ_df.insert(0, key, info[key])

    file_util.saveinfo(summ_df, "scores_df.csv", dirname)

    return summ_df


#############################################
def setup_run(extrapar, techpar, sess_data, comp="unexp", gab_ori="all"):
    """
    setup_run(extrapar, techpar, sess_data)
    
    Sets up run(s) by setting seed, getting classes and number of ROIs.

    Required args:
        - extrapar (dict)      : dictionary with extra parameters
            ["seed"] (int)    : seed to use
            ["shuffle"] (bool): if analysis is on shuffled data
            ["uniqueid"] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ["alg"] (str)      :  algorithm used ("sklearn" or "pytorch")
            ["compdir"] (str)  : specific output comparison directory
            ["device"] (str)   : device to use (e.g., "cpu" or "cuda")
            ["fontdir"] (str)  : directory in which additional fonts are 
                                 located
            ["output"] (str)   : main output directiory
            ["plt_bkend"] (str): plt backend to use (e.g., "agg" or None)
            ["reseed"] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
            - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_unexps (list)    : list of lists of number of unexpected sequences
                                  (doubled if "half" comparison), 
                                  structured as datasets x class 

    Optional args:
        - comp (str)           : comparison
                                 default: "unexp"
        - gab_ori (str or list): Gabor orientations,
                                 for comp values like DoriU, DoriA, etc.
                                 default: "all"

    Returns:
        - extrapar (dict)   : dictionary with extra parameters
            ["classes"] (list): list of class names
            ["n_rois"] (int)  : number of ROIs
            ["seed"] (int)    : seed to use
            ["shuffle"] (bool): if analysis is on shuffled data
            ["uniqueid"] (str): unique ID for the analysis
        - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                              (1 or 2 if an additional test set is included), 
                              each structured as sequences x frames x ROIs
        - seq_classes (list): list of 2D arrays of sequence classes
                              (1 or 2 if an additional test set is included), 
                              each structured as class values x 1
        - n_unexps (list)    : list of lists of number of unexpected sequences
                              (doubled if "half" comparison),
                              structured as datasets x class

    """

    extrapar = copy.deepcopy(extrapar)
    if techpar["reseed"]: # reset seed         
        extrapar["seed"] = None
    extrapar["seed"] = rand_util.seed_all(
        extrapar["seed"], techpar["device"], 
        seed_torch=(techpar["alg"] == "pytorch")) # seed torch, if needed
    extrapar["classes"] = get_classes(comp, gab_ori)
    
    [roi_seqs, seq_classes, n_unexps] = copy.deepcopy(sess_data)
    extrapar["n_rois"]  = roi_seqs[0].shape[-1]

    return extrapar, roi_seqs, seq_classes, n_unexps


#############################################
def all_runs_sk(n_runs, analyspar, logregpar, sesspar, stimpar, extrapar, 
                techpar, sess_data):
    """
    all_runs_sk(n_runs, analyspar, logregpar, sesspar, stimpar, extrapar, 
                techpar, sess_data)

    Does all runs of a logistic regression on the specified comparison
    and session data using sklearn. Records hyperparameters, all models,  
    tr_stats dictionary. Plots scores and training statistics. 
    
    Required args:
        - n_runs (int)         : number of runs to do
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ["seed"] (int)    : seed to use
            ["shuffle"] (bool): if analysis is on shuffled data
            ["uniqueid"] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ["compdir"] (str) : specific output comparison directory
            ["device"] (str)   : device to use (e.g., "cpu" or "cuda")
            ["fontdir"] (str)  : directory in which additional fonts are 
                                 located
            ["output"] (str)   : main output directiory
            ["plt_bkend"] (str): plt backend to use (e.g., "agg" or None)
            ["reseed"] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)   : list of 3D arrays of selected ROI trace seqs 
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
            - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_unexps (list)    : list of lists of number of unexpected sequences
                                  (doubled if "half" comparison), 
                                  structured as datasets x class
    """
    logregpar = sess_ntuple_util.get_modif_ntuple(
        logregpar, ["batchsize", "lr", "wd"], [None, None, None])
    if techpar["device"] == "cuda":
        warnings.warn(
            "sklearn method not implemented with GPU.", 
            category=RuntimeWarning, stacklevel=1
            )

    [extrapar, roi_seqs, seq_classes, n_unexps] = setup_run(
         extrapar, techpar, sess_data, logregpar.comp, gab_ori=stimpar.gab_ori
         )
    main_data = [roi_seqs[0], seq_classes[0]]

    samples = [False for _ in n_unexps]
    if logregpar.ctrl:
        # get n_unexp for last class
        samples = [n_unexp[-1] for n_unexp in n_unexps]
        keep_all = [val in logregpar.comp for val in ["ori", "dir_exp", "half"]]
        if sum(keep_all) > 0: # get n_unexp for all classes
            samples = n_unexps

    plot_util.manage_mpl(techpar["plt_bkend"], fontdir=techpar["fontdir"])

    extrapar = copy.deepcopy(extrapar)
    extrapar["n_runs"] = n_runs
    rundir = get_rundir(extrapar["n_runs"], extrapar["uniqueid"], logregpar.alg)
    extrapar["dirname"] = str(file_util.createdir([techpar["output"], 
        techpar["compdir"], rundir]))
    
    scale_str = sess_str_util.scale_par_str(analyspar.scale, "print")
    shuff_str = sess_str_util.shuff_par_str(extrapar["shuffle"], "labels")

    logger.info(f"Runs ({n_runs}): {scale_str}{shuff_str}", 
        extra={"spacing": "\n"})

    split_test = False

    returns = logreg_util.run_logreg_cv_sk(
        main_data[0], main_data[1], logregpar._asdict(), extrapar, 
        analyspar.scale, samples[0], split_test, extrapar["seed"], 
        techpar["parallel"], catch_set_prob=True)
    if returns is None:
        return
    else:
        mod_cvs, cv, extrapar = returns


    hyperpars = save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    # Additional data for scoring
    set_names  = ["train", "test"]
    extra_data, extra_cv = None, None
    extra_name = sess_str_util.ext_test_str(
        logregpar.q1v4, logregpar.exp_v_unexp, logregpar.comp)
    if cv._split_test:
        set_names.append("test_out")
    if len(roi_seqs) == 2:
        extra_data = [roi_seqs[1], seq_classes[1]]
        extra_cv = logreg_util.StratifiedShuffleSplitMod(
            n_splits=cv.n_splits, train_p=0.5, sample=samples[1], 
            bal=logregpar.bal) # since train_p cannot be 1.0
        if extra_name is None:
            raise RuntimeError("Extra test dataset not labelled.")
        set_names.append(extra_name)

    mod_cvs = logreg_util.test_logreg_cv_sk(
        mod_cvs, cv, extrapar["scoring"], main_data=main_data, 
        extra_data=extra_data, extra_name=extra_name, extra_cv=extra_cv, 
        catch_set_prob=True)
    if mod_cvs is None:
        return

    # Get and save best model
    best_mod_idx = np.argmax(mod_cvs[f"{set_names[-1]}_neg_log_loss"])
    best_mod     = mod_cvs["estimator"][best_mod_idx]
    # Save data used for best model
    plot_data, plot_targ, data_names = [], [], []
    for name, subcv, data in zip(
        ["train", extra_name], [cv, extra_cv], [main_data, extra_data]):
        if data is None:
            continue
        if name == "train":
            idx = subcv._set_idx[best_mod_idx][0]
        else:
            idx = [i for sub in subcv._set_idx[best_mod_idx] for i in sub]
        plot_data.append(data[0][idx])
        plot_targ.append(data[1][idx])
        data_names.append(name)

    tr_stats = save_tr_stats(
        plot_data, plot_targ, data_names, analyspar, stimpar, 
        extrapar["n_rois"], logregpar.alg, best_mod, 
        dirname=extrapar["dirname"])
   
    # Get scores dataframe
    scores = logreg_util.create_score_df_sk(
        mod_cvs, best_mod_idx, set_names, extrapar["scoring"])
    info = info_dict(
        analyspar, sesspar, stimpar, extrapar, logregpar.comp, logregpar.alg, 
        n_rois=extrapar["n_rois"])
    full_scores = save_scores(
        info, scores, key_order=info_dict(), dirname=extrapar["dirname"])

    # Save best model
    logreg_plots.plot_traces_scores(
        hyperpars, tr_stats, full_scores, plot_wei=best_mod_idx)

    plot_util.cond_close_figs()



#############################################
def single_run_pt(run_n, analyspar, logregpar, sesspar, stimpar, extrapar, 
                  techpar, sess_data):
    """
    single_run_pt(run_n, analyspar, logregpar, sesspar, stimpar, extrapar, 
                  techpar, sess_data)

    Does a single run of a logistic regression using PyTorch on the specified 
    comparison and session data. Records hyperparameters, best model, last 
    model, tr_stats dictionary. Plots scores and training statistics. 
    
    Required args:
        - run_n (int)          : run number
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 parameters
        - sesspar (SessPar)    : named tuple containing session parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - extrapar (dict)      : dictionary with extra parameters
            ["seed"] (int)    : seed to use
            ["shuffle"] (bool): if analysis is on shuffled data
            ["uniqueid"] (str): unique ID for the analysis
        - techpar (dict)       : dictionary with technical parameters
            ["compdir"] (str) : specific output comparison directory
            ["device"] (str)   : device to use (e.g., "cpu" or "cuda")
            ["fontdir"] (str)  : directory in which additional fonts are 
                                 located
            ["output"] (str)   : main output directiory
            ["plt_bkend"] (str): plt backend to use (e.g., "agg" or None)
            ["reseed"] (bool)  : if True, run is reseeded
        - sess_data (list): list of session data:
            - roi_seqs (list)  : list of 3D arrays of selected ROI trace seqs
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      sequences x frames x ROIs
           - seq_classes (list): list of 2D arrays of sequence classes
                                  (1 or 2 if an additional test set is 
                                  included), each structured as 
                                      class values x 1
            - n_unexps (list)    : number of unexpected sequences, listed by 
                                  quantile, (doubled if "half" comparison)
    """

    from util import data_util # avoid torch dependency for the whole module
    
    extrapar = copy.deepcopy(extrapar)
    extrapar["seed"] *= run_n + 1 # ensure different seed for each run

    [extrapar, roi_seqs, seq_classes, n_unexps] = setup_run(
        extrapar, techpar, sess_data, logregpar.comp, gab_ori=stimpar.gab_ori
        )

    extrapar["run_n"] = run_n
    scale_str = sess_str_util.scale_par_str(analyspar.scale, "print")
    shuff_str = sess_str_util.shuff_par_str(extrapar["shuffle"], "labels")
    run_n = extrapar["run_n"]

    logger.info(f"Run: {run_n}{scale_str}{shuff_str}", 
        extra={"spacing": "\n"})

    for i in range(len(roi_seqs)):
        if logregpar.ctrl: # select a random subsample
            roi_seqs[i], seq_classes[i] = sample_seqs(
                roi_seqs[i], seq_classes[i], n_unexps[i][-1])  
        if logregpar.bal: # balance classes
            roi_seqs[i], seq_classes[i] = data_util.bal_classes(
                roi_seqs[i], seq_classes[i])

    plot_util.manage_mpl(techpar["plt_bkend"], fontdir=techpar["fontdir"])
    
    thresh_cl = 2
    if sesspar.runtype == "pilot":
        thresh_cl = 1

    rundir = get_rundir(extrapar["run_n"], extrapar["uniqueid"], logregpar.alg)
    extrapar["dirname"] = str(file_util.createdir(
        [techpar["output"], techpar["compdir"], rundir]))

    returns = init_logreg_model_pt(
        roi_seqs, seq_classes, logregpar, extrapar, analyspar.scale, 
        techpar["device"], thresh_cl=thresh_cl, catch_set_prob=True)
    if returns is None:
        return
    else:
        mod, dls, extrapar = returns

    hyperpars = save_hyperpar(analyspar, logregpar, sesspar, stimpar, extrapar)

    data_names = ["train"]
    extra_name = sess_str_util.ext_test_str(
        logregpar.q1v4, logregpar.exp_v_unexp, logregpar.comp)
    idx = [0]
    if len(roi_seqs) == 2:
        idx.append(-1)
        if extra_name == "":
            raise RuntimeError("Extra test dataset not labelled.")
        data_names.append(extra_name)
    plot_data = [dls[i].dataset.data for i in idx]
    plot_targ = [dls[i].dataset.targets for i in idx]

    tr_stats = save_tr_stats(
        plot_data, plot_targ, data_names, analyspar, stimpar, 
        extrapar["n_rois"], alg=logregpar.alg, dirname=extrapar["dirname"])

    info = info_dict(
        analyspar, sesspar, stimpar, extrapar, logregpar.comp, logregpar.alg, 
        n_rois=tr_stats["n_rois"])
    scores = logreg_util.fit_model_pt(
        info, logregpar.n_epochs, mod, dls, techpar["device"], 
        extrapar["dirname"], ep_freq=techpar["ep_freq"], 
        test_dl2_name=extra_name)

    logger.info(f"Run {extrapar['run_n']}: training done.\n")

    # save scores in dataframe
    full_scores = save_scores(
        info, scores, key_order=info_dict(), dirname=extrapar["dirname"])

    # plot traces and scores (only for first 5 runs)
    # no plotting for the rest to reduce number of files generated
    if run_n <= 5:
        logreg_plots.plot_traces_scores(
            hyperpars, tr_stats, full_scores, plot_wei=True)

    plot_util.cond_close_figs()


#############################################
def run_regr(sess, analyspar, stimpar, logregpar, quantpar, extrapar, techpar):
    """
    Does runs of a logistic regressions on the specified comparison on a 
    session.

    Required args:
        - sess (Session)       : Session on which to run logistic regression
        - analyspar (AnalysPar): named tuple containing analysis parameters
        - stimpar (StimPar)    : named tuple containing stimulus parameters
        - logregpar (LogRegPar): named tuple containing logistic regression 
                                 analysis parameters
        - quantpar (QuantPar)  : named tuple containing quantile analysis 
                                 parameters
        - extrapar (dict)      : dictionary containing additional analysis 
                                 parameters
            ["uniqueid"] (str or int): unique ID for analysis
            ["seed"] (int)           : seed to seed random processes with
        - techpar (dict)       : dictionary containing technical analysi 
                                 parameters
            ["device"] (str)   : device name (i.e., "cuda" or "cpu")
            ["ep_freq"] (int)  : frequency at which to log loss to console
            ["fontdir"] (str)  : directory in which additional fonts are 
                                 located
            ["n_reg"] (int)    : number of expected runs
            ["n_shuff"] (int)  : number of shuffled runs
            ["output"] (str)   : general directory in which to save output
            ["parallel"] (bool): if True, runs are done in parallel
            ["plt_bkend"] (str): pyplot backend to use
            ["reseed"] (bool)  : if True, each run is reseeded
    """

    techpar = copy.deepcopy(techpar)
    sesspar = sess_ntuple_util.init_sesspar(
        sess.sess_n, False, sess.plane, sess.line, runtype=sess.runtype, 
        mouse_n=sess.mouse_n)

    class_var, unexps = get_class_pars(logregpar.comp, stimpar.stimtype)
    split_oris = sess_str_util.get_split_oris(logregpar.comp)

    try:
        sess_data = get_sess_data(
            sess, analyspar, stimpar, quantpar, class_var, unexps, 
            exp_v_unexp=logregpar.exp_v_unexp, split_oris=split_oris)
    except RuntimeError as err:
        catch_phr = ["No frames", "No segments", "Some quantiles are empty", 
            "Segments do not"]
        catch = sum(phr in str(err) for phr in catch_phr)
        if catch:            
            warnings.warn(str(err), category=RuntimeWarning, stacklevel=1)
            return
        else:
            raise err

    for n_runs, shuffle in zip(
        [techpar["n_reg"], techpar["n_shuff"]], [False, True]):
        if n_runs == 0:
            continue
        extrapar = copy.deepcopy(extrapar)
        extrapar["shuffle"] = shuffle
        techpar = copy.deepcopy(techpar)
        techpar["compdir"] = str(sess_gen_util.get_analysdir(
            sesspar.mouse_n, sesspar.sess_n, sesspar.plane, analyspar.fluor, 
            analyspar.scale, stimpar.stimtype, stimpar.visflow_dir, 
            stimpar.visflow_size, stimpar.gabk, logregpar.comp, logregpar.ctrl, 
            extrapar["shuffle"]))
        if logregpar.alg == "pytorch":
            techpar["compdir"] = f"{techpar['compdir']}_pt"
            if n_runs == 0:
                continue
            # optionally runs in parallel
            args_list = [analyspar, logregpar, sesspar, stimpar, extrapar, 
                techpar, sess_data]
            gen_util.parallel_wrap(
                single_run_pt, range(n_runs), args_list, 
                parallel=techpar["parallel"])
        elif logregpar.alg == "sklearn":
            all_runs_sk(n_runs, analyspar, logregpar, sesspar, stimpar, 
                extrapar, techpar, sess_data)
        else:
            gen_util.accepted_values_error("logregpar.alg", logregpar.alg, 
                ["pytorch", "sklearn"])


#############################################
def collate_scores(direc, all_labels, alg="sklearn"):
    """
    collate_scores(direc, all_labels)

    Collects the analysis information and scores from the last epoch recorded 
    for a run and returns in dataframe.
    
    Required args:
        - direc (str)      : path to the specific comparison run folder
        - all_labels (list): ordered list of columns to save to dataframe

    Optional args:
        - alg (str): algorithm used to run logistic regression 
                     ("sklearn" or "pytorch")
                     default: "sklearn"

    Return:
        - scores (pd DataFrame): Dataframe containing run analysis information
                                 and scores from the last epoch recorded.
    """

    logger.info(direc)
 
    scores = pd.DataFrame(columns=all_labels)

    ep_info, hyperpars = logreg_util.get_scores(direc, alg)
    if ep_info is None:
        comp_dict = get_compdir_dict(direc, no_lists=True)
        comp_dict["scale"] = hyperpars["analyspar"]["scale"]
        comp_dict["runtype"] = hyperpars["sesspar"]["runtype"]
        comp_dict["line"] = hyperpars["sesspar"]["line"]
        comp_dict["n_rois"] = hyperpars["extrapar"]["n_rois"]
        for col in all_labels: # ensures correct order
            if col in comp_dict.keys():
                scores.loc[0, col] = comp_dict[col]
    else:
        for col in all_labels:
            if col in ep_info.columns:
                if alg == "pytorch":
                    scores.loc[0, col] = ep_info[col].item()
                elif alg == "sklearn":
                    scores[col] = ep_info[col]

    return scores


#############################################
def remove_overlap_comp_dir(gen_dirs, stimtype="gabors", comp="unexp"):
    """
    remove_overlap_comp_dir(gen_dirs)

    Returns list of directories with those corresponding to comparisons with 
    overlapping names removed.

    Required args:
        - gen_dirs (list): list of directories

    Optional args:
        - stimtype (str) : stimulus type
                           default: "gabors"
        - comp (str)     : type of comparison
                           default: "unexp"
    """

    all_comps = get_comps(stimtype)

    for other_comp in all_comps:
        if comp != other_comp and comp in other_comp:
            gen_dirs = [gen_dir for gen_dir in gen_dirs 
                if other_comp not in str(gen_dir)]

    return gen_dirs


#############################################
def adjust_duplicate_runs(all_scores):
    """
    adjust_duplicate_runs(all_scores)

    Adjust the run numbers for duplicate runs so that they are sequential and 
    not repeating.

    Required args:
        - all_scores (pd DataFrame): dataframe compiling all the scores for 
                                     logistic regression runs


    Returns:
        - all_scores (pd DataFrame): dataframe compiling all the scores for 
                                     logistic regression runs, with run numbers 
                                     adjusted if needed to be sequential and 
                                     non repeating.
    """

    grouping_keys = list(filter(
        lambda x: x not in ["uniqueid", "run_n", "epoch_n"], info_dict()))

    # runs are expected to occur sequentially without gaps in the dataframe 
    # for any grouping. 
    for _, group in all_scores.groupby(grouping_keys):
        breaks = group.loc[group["run_n"].diff() < 0].index
        for break_pt in breaks:
            current_max = group.loc[0 : break_pt, "run_n"].max()
            group.loc[break_pt :, "run_n"] += current_max + 1
        if len(breaks):
            all_scores.loc[group.index, "run_n"] = group["run_n"]

    return all_scores


#############################################
def run_collate(output, stimtype="gabors", comp="unexp", ctrl=False, 
                alg="sklearn", parallel=False):
    """
    run_collate(output)

    Collects the analysis information and scores from the last epochs recorded 
    for all runs for a comparison type, and saves to a dataframe.

    Overwrites any existing dataframe of collated data.  
    
    Required args:
        - output (Path): general directory in which summary dataframe is saved
    
    Optional args:
        - stimtype (str) : stimulus type
                           default: "gabors"
        - comp (str)     : type of comparison
                           default: "unexp"
        - ctrl (bool)    : if True, control comparisons are analysed
                           default: False
        - alg (str)      : algorithm used to run logistic regression 
                           ("sklearn" or "pytorch")
                           default: "sklearn"
        - parallel (bool): if True, run information is collected in parallel
                           default: False

    Returns:
        - all_scores (pd DataFrame): dataframe compiling all the scores for 
                                     logistic regression runs in the output 
                                     folder that correspond to the stimulus 
                                     type and comparison type criteria
    """

    output = Path(output)
    
    if not output.is_dir():
        logger.info(f"{output} does not exist.")
        return

    ext_test = sess_str_util.ext_test_str(
        ("_q1v4" in str(output)), ("_evu" in str(output)), comp)
    if ext_test == "":
        ext_test = None

    ctrl_str = sess_str_util.ctrl_par_str(ctrl)
    stim_str = "gab" if stimtype == "gabors" else stimtype
    gen_dirs = file_util.getfiles(
        output, "subdirs", [stim_str, comp, ctrl_str])
                         
    if alg == "sklearn":
        gen_dirs = [gen_dir for gen_dir in gen_dirs if "_pt" not in str(gen_dir)]
    elif alg == "pytorch":
        gen_dirs = [gen_dir for gen_dir in gen_dirs if "_pt" in str(gen_dir)]
    else:
        gen_util.accepted_values_error("alg", alg, ["sklearn", "pytorch"])

    gen_dirs = remove_overlap_comp_dir(gen_dirs, stimtype, comp)
    if not ctrl:
        gen_dirs = [gen_dir for gen_dir in gen_dirs 
            if "ctrl" not in str(gen_dir)]

    if len(gen_dirs) == 0:
        logger.info("No runs found.")
        return

    run_dirs = [run_dir for gen_dir in gen_dirs
        for run_dir in file_util.getfiles(gen_dir, "subdirs")]

    # identify, remove and flag empty directories     
    empty_dirs = [run_dir for run_dir in run_dirs 
        if len(list(run_dir.iterdir())) == 0]

    if len(empty_dirs) != 0:
        logger.info("EMPTY DIRECTORIES:")
        for empty_dir in empty_dirs:
            run_dirs.remove(empty_dir)
            logger.info(f"{empty_dir}", extra={"spacing": TAB})

    all_labels = info_dict() + \
        logreg_util.get_sc_labs(True, ext_test_name=ext_test) + ["saved"]

    scores_list = gen_util.parallel_wrap(
        collate_scores, run_dirs, [all_labels, alg], parallel=parallel)
    # check for repeated run numbers
    if len(scores_list) != 0:
        all_scores = pd.concat(scores_list)
        all_scores = all_scores[all_labels] # reorder
    else:
        all_scores = pd.DataFrame(columns=all_labels)

    # check for and adjust duplicate run numbers
    all_scores = all_scores.reset_index(drop=True)
    all_scores = adjust_duplicate_runs(all_scores)

    # sort df by mouse, session, plane, line, fluor, scale, shuffle, stimtype,
    # comp, uniqueid, run_n, runtype
    sorter = info_dict()[0:13]    

    all_scores = all_scores.sort_values(by=sorter).reset_index(drop=True)

    savename = get_df_name("collate", stimtype, comp, ctrl, alg)
    file_util.saveinfo(all_scores, savename, output, overwrite=True)

    return all_scores


#############################################
def calc_stats(scores_summ, curr_lines, curr_idx, CI=0.95, ext_test=None, 
               stats="mean", shuffle=False):
    """
    calc_stats(scores_summ, curr_lines, curr_idx)

    Calculates statistics on scores from runs with specific analysis criteria
    and records them in the summary scores dataframe.  
    
    Required args:
        - scores_summ (pd DataFrame): DataFrame containing scores summary
        - curr_lines (pd DataFrame) : DataFrame lines corresponding to specific
                                      analysis criteria
        - curr_idx (int)            : Current row in the scores summary 
                                      DataFrame 
    
    Optional args:
        - CI (num)        : Confidence interval around which to collect 
                            percentile values
                            default: 0.95
        - extra_test (str): Name of extra test set, if any (None if none)
                            default: None
        - stats (str)     : stats to take, i.e., "mean" or "median"
                            default: "mean"
        - shuffle (bool)  : If True, data is for shuffled, and will be averaged 
                            across runs before taking stats
                            default: False

    Returns:
        - scores_summ (pd DataFrame): Updated DataFrame containing scores, as
                                      well as epoch_n, runs_total, runs_nan 
                                      summaries
    """

    scores_summ = copy.deepcopy(scores_summ)

    # score labels to perform statistics on
    sc_labs = ["epoch_n"] + logreg_util.get_sc_labs(
        True, ext_test_name=ext_test)

    # avoids accidental nuisance dropping by pandas
    curr_lines["epoch_n"] = curr_lines["epoch_n"].astype(float)

    if shuffle: # group runs and take mean or median across
        scores_summ.loc[curr_idx, "mouse_n"] = -1
        keep_lines = \
            [col for col in curr_lines.columns if col in sc_labs] + ["run_n"]
        grped_lines = curr_lines[keep_lines].groupby("run_n", as_index=False)
        if stats == "mean":
            curr_lines = grped_lines.mean() # automatically skips NaNs
        elif stats == "median":
            curr_lines = grped_lines.median() # automatically skips NaNs
        else:
            gen_util.accepted_values_error("stats", stats, ["mean", "median"])

    # calculate n_runs (without nans and with)
    scores_summ.loc[curr_idx, "runs_total"] = len(curr_lines)
    scores_summ.loc[curr_idx, "runs_nan"] = curr_lines["epoch_n"].isna().sum()


    # percentiles to record
    ps, p_names = math_util.get_percentiles(CI)

    for sc_lab in sc_labs:
        if sc_lab in curr_lines.keys():
            cols = []
            vals = []
            data = curr_lines[sc_lab].astype(float)
            for stat in ["mean", "median"]:
                cols.extend([stat])
                vals.extend(
                    [math_util.mean_med(data, stats=stat, nanpol="omit")])
            for error in ["std", "sem"]:
                cols.extend([error])
                vals.extend([math_util.error_stat(
                    data, stats="mean", error=error, nanpol="omit")])
            # get 25th and 75th quartiles
            cols.extend(["q25", "q75"])
            vals.extend(math_util.error_stat(
                data, stats="median", error="std", nanpol="omit"))                                            
            # get other percentiles (for CI)
            cols.extend(p_names)
            vals.extend(math_util.error_stat(
                data, stats="median", error="std", nanpol="omit", qu=ps))
            
            # get MAD
            cols.extend(["mad"])
            vals.extend([math_util.error_stat(
                data, stats="median", error="sem", nanpol="omit")])

            # plug in values
            cols = [f"{sc_lab}_{name}" for name in cols]
            scores_summ = gen_util.set_df_vals(
                scores_summ, curr_idx, cols, vals, in_place=True
                )

    return scores_summ


#############################################
def run_analysis(output, stimtype="gabors", comp="unexp", ctrl=False, 
                 CI=0.95, alg="sklearn", parallel=False, all_scores_df=None):  
    """
    run_analysis(output)

    Calculates statistics on scores from runs for each specific analysis 
    criteria and saves them in the summary scores dataframe.

    Overwrites any existing dataframe of analysed data.

    Required args:
        - output (Path): general directory in which summary dataframe is saved.
    
    Optional args:
        - stimtype (str)       : stimulus type
                                 default: "gabors"
        - comp (str)           : type of comparison
                                 default: "unexp"
        - ctrl (bool)          : if True, control comparisons are analysed
                                 default: False
        - CI (num)             : CI for shuffled data
                                 default: 0.95
        - alg (str)            : algorithm used to run logistic regression 
                                 ("sklearn" or "pytorch")
                                 default: "sklearn"
        - parallel (bool)      : if True, run information is collected in 
                                 parallel
                                 default: False
        - all_scores_df (pd df): already collated scores dataframe
                                 default: None
    
    Returns:
        - scores_summ (pd DataFrame): dataframe with analysed scores
    """

    if all_scores_df is None:
        all_scores_df = run_collate(output, stimtype, comp, ctrl, alg, parallel)

    stats = "mean" # across runs for shuffle CIs

    if all_scores_df is None:
        return

    scores_summ = pd.DataFrame()

    ext_test = sess_str_util.ext_test_str(
        ("_q1v4" in str(output)), ("_evu" in str(output)), comp)
    if ext_test == "":
        ext_test = None

    # common labels
    comm_labs = gen_util.remove_if(info_dict(), 
        ["uniqueid", "run_n", "epoch_n"])

    # get all unique comb of labels
    for acr_shuff in [False, True]:
        if not acr_shuff:
            df_unique = all_scores_df[comm_labs].drop_duplicates()
        else:
            df_unique = all_scores_df[gen_util.remove_if(comm_labs, 
                ["mouse_n", "n_rois"])].drop_duplicates()
        for _, df_row in df_unique.iterrows():
            if acr_shuff and not df_row["shuffle"]:
                # second pass, only shuffle
                continue
            vals = [df_row[x] for x in comm_labs]
            curr_lines = gen_util.get_df_vals(all_scores_df, comm_labs, vals)
            # assign values to current line in summary df
            curr_idx = len(scores_summ)
            scores_summ = gen_util.set_df_vals(
                scores_summ, curr_idx, comm_labs, vals, in_place=True
                )
            # calculate stats
            scores_summ = calc_stats(scores_summ, curr_lines, curr_idx, CI, 
                ext_test, stats=stats, shuffle=acr_shuff)
    
    savename = get_df_name("analyse", stimtype, comp, ctrl, alg)
    file_util.saveinfo(scores_summ, savename, output, overwrite=True)
    
    return scores_summ


#############################################    
def run_plot(output, stimtype="gabors", comp="unexp", ctrl=False, 
             visflow_dir="both", fluor="dff", scale=True, CI=0.95, 
             alg="sklearn", plt_bkend=None, fontdir=None, modif=False):
    """
    run_plot(output)

    Plots summary data for a specific comparison, for each datatype in a 
    separate figure and saves figures. 

    Required args:
        - output (Path): general directory in which summary dataframe is saved
    
    Optional args:
        - stimtype (str)   : stimulus type
                             default: "gabors"
        - comp (str)       : type of comparison
                             default: "unexp"
        - ctrl (bool)      : if True, control comparisons are analysed
                             default: False
        - visflow_dir (str): visual flow direction
                             default: "both"
        - fluor (str)      : fluorescence trace type
                             default: "dff"
        - scale (bool)     : whether data is scaled by ROI
                             default: True
        - CI (num)         : CI for shuffled data
                             default: 0.95
        - alg (str)        : algorithm used to run logistic regression 
                             ("sklearn" or "pytorch")
                             default: "sklearn"
        - plt_bkend (str)  : pyplot backend to use
                             default: None
        - fontdir (str)    : directory in which additional fonts are located
                             default: None
        - modif (bool)     : if True, plots are made in a modified (simplified 
                             way)
                             default: False
    """

    if comp in ["half_right", "half_left"]:
        visflow_dir = comp.replace("half_", "")

    savename = get_df_name("analyse", stimtype, comp, ctrl, alg)

    logreg_plots.plot_summ(output, savename, stimtype, comp, ctrl, visflow_dir, 
        fluor, scale, CI, plt_bkend, fontdir, modif)

