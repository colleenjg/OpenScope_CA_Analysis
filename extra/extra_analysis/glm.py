"""
glm.py

This module contains functions to run and analyse GLMs predicting ROI activity
from stimulus and behavioural information for data generated by the Allen 
Institute OpenScope experiments for the Credit Assignment Project.

Authors: Colleen Gillon

Date: September, 2019

Note: this code uses python 3.7.

"""

import copy
import logging
import random

from joblib import Parallel, delayed
import numpy as np
import pandas as pd
from sklearn import linear_model, metrics, model_selection, pipeline, \
    preprocessing

from util import file_util, gen_util, logger_util, math_util, rand_util
from sess_util import sess_data_util, sess_gen_util, sess_str_util
from extra_plot_fcts import glm_plots

logger = logging.getLogger(__name__)


#############################################
def build_stim_beh_df(sessions, analyspar, sesspar, stimpar, each_roi=False):
    """
    build_stim_beh_df(sessions, stimpar)

    Builds a dataframe containing the stimulus and behavioural information, for 
    each of the specified segments.

    Inputs: sessions, stimtype, segments, pre-post (pupil, running), pre-post 
    (ROI)

    GLM inputs:
    - (Segments (A, B, C, D))
    - Surprise status (only 1 for U and the following grayscreen (G))
    - Pupil dilation
    - Running velocity
    - Plane
    - Line
    - SessID

    """

    full_df = pd.DataFrame()
    
    sessions = gen_util.list_if_not(sessions)
    for sess in sessions:
        logger.info(f"Building dataframe for session {sess.sessid}", 
            extra={"spacing": "\n"})
        stim = sess.get_stim(stimpar.stimtype)
        sub_df = stim.get_stim_beh_sub_df(
            stimpar.pre, stimpar.post, analyspar.stats, analyspar.fluor, 
            analyspar.remnans, stimpar.gabfr, stimpar.gabk, stimpar.gab_ori, 
            stimpar.bri_size, stimpar.bri_dir, pupil=True, run=True)

        full_df = full_df.append(sub_df)
    
    full_df = full_df.reset_index(drop=True)
    full_df = gen_util.drop_unique(full_df, in_place=True)

    return full_df


#############################################
def log_sklearn_results(model, analyspar, name="bayes_ridge", var_names=None):

    
    logger.info(f"{name.replace('_', ' ').upper()} regression", 
        extra={"spacing": "\n"})
    if var_names is None:
        var_names = [f"coef {i}" for i in range(len(model.coef_))] 

    results = "\n".join([f"{varn}: {coef:.5f}" for varn, coef 
        in zip(var_names, model.coef_)])
    logger.info(results)
    logger.info(f"intercept: {model.intercept_:.5f}")
    logger.info(f"alpha: {model.alpha_:.5f}", extra={"spacing": "\n"})
    if name == "ridge_cv":
        alpha_idx = np.where(model.alphas == model.alpha_)[0]
        score_data = model.cv_values_[:, alpha_idx]
        score_name = "MSE"
    elif name == "bayes_ridge":
        score_data = model.scores_
        score_name = "Score"
    else:
        gen_util.accepted_values_error(
            "name", name, ["ridge_cv", "bayes_ridge"])
    stats = math_util.get_stats(
        score_data, stats=analyspar.stats, error=analyspar.error)
    math_util.log_stats(stats, f"{score_name}")


#############################################
def run_ridge_cv(x_df, y_df, analyspar, alphas=None):

    if alphas is None:
        alphas=(0.1, 0.1, 2.0)

    steps = [("scaler", preprocessing.StandardScaler()), 
        ("model", linear_model.RidgeCV(
            normalize=True, alphas=alphas, store_cv_values=True))] 

    pipl = pipeline.Pipeline(steps)
    pipl.fit(x_df, y_df)

    log_sklearn_results(
        pipl, analyspar, name="ridge_cv", var_names=x_df.columns)

#############################################
def run_bayesian_ridge(x_df, y_df, analyspar):

    steps = [("scaler", preprocessing.StandardScaler()), 
        ("model", linear_model.BayesianRidge(compute_score=True))] 
    pipl = pipeline.Pipeline(steps)
    pipl.fit(x_df, y_df)

    log_sklearn_results(
        pipl, analyspar, name="bayes_ridge", var_names=x_df.columns)


#############################################
def fit_expl_var(x_df, y_df, train_idx, test_idx, stimpar):

    x_df = sess_data_util.add_categ_stim_cols(copy.deepcopy(x_df))

    x_tr, y_tr     = x_df.loc[train_idx], y_df.loc[train_idx].to_numpy().ravel()
    x_test, y_test = x_df.loc[test_idx], y_df.loc[test_idx].to_numpy().ravel()

    # get explained variance
    steps = [("scaler", preprocessing.StandardScaler()), 
        ("model", linear_model.BayesianRidge(compute_score=True))] 
    pipl = pipeline.Pipeline(steps)
    pipl.fit(x_tr, y_tr)
    y_pred = pipl.predict(x_test)
    varsc = metrics.explained_variance_score(
        y_test, y_pred, multioutput="uniform_average")
    return varsc


#############################################
def is_bool_var(df_col):
    if set(df_col.unique()).issubset({0, 1}):
        is_bool = True
    else:
        is_bool = False

    return is_bool


#############################################
def get_categ(col_name):
    categ_symb = "$"
    if categ_symb in col_name:
        categ_name = col_name[ : col_name.find(categ_symb)]
    else:
        categ_name = col_name
    return categ_name


#############################################
def shuffle_col_idx(full_df, col, idx=None):
    full_df = copy.deepcopy(full_df)
    if idx is None:
        other_vals = full_df[col].tolist()
        random.shuffle(other_vals)
        full_df[col] = other_vals    
    else:
        other_vals = full_df.loc[idx, col].tolist()
        random.shuffle(other_vals)
        full_df.loc[idx, col] = other_vals
    return full_df


#############################################
def fit_expl_var_per_coeff(x_df, y_df, train_idx, test_idx, stimpar):

    # get df with categorical variables split up
    x_df_cat = sess_data_util.add_categ_stim_cols(copy.deepcopy(x_df))
    
    x_cols     = x_df.columns
    x_cols_cat = x_df_cat.columns

    done = []
    expl_var = dict()
    for col in x_cols_cat:
        for run in ["full_categ", "single_categ_value"]:
            curr_x_df = copy.deepcopy(x_df)
            act = col
            key = col
            # categorical variables
            if is_bool_var(x_df_cat[col]):
                categ_name = get_categ(col)
                act = categ_name
                if run == "full_categ":
                    key = categ_name
                    if categ_name in done or categ_name == col:
                        continue
                    else:
                        done.append(categ_name)
                # shuffle the other instances of category
                elif run == "single_categ_value":
                    act_idx_tr = (x_df_cat.loc[train_idx, col] == 0)
                    curr_x_df.loc[train_idx] = shuffle_col_idx(
                        curr_x_df.loc[train_idx], categ_name, act_idx_tr)

            # shuffle all other columns
            for oth_col in x_cols:
                if oth_col != act:
                    curr_x_df.loc[train_idx] = shuffle_col_idx(
                        curr_x_df.loc[train_idx], oth_col)

            expl_var[key] = fit_expl_var(
                curr_x_df, y_df, train_idx, test_idx, stimpar)
    
    return expl_var


#############################################
def fit_unique_expl_var_per_coeff(x_df, y_df, train_idx, test_idx, stimpar, 
                                  full_expl_var):

    # get df with categorical variables split up
    x_df_cat = sess_data_util.add_categ_stim_cols(copy.deepcopy(x_df))
    x_cols_cat = x_df_cat.columns

    expl_var = dict()
    done = []
    for col in x_cols_cat:
        if is_bool_var(x_df_cat[col]):
            categ_name = get_categ(col)
            if categ_name in done:
                continue
            else:
                done.append(categ_name)
                col = categ_name

        curr_x_df = copy.deepcopy(x_df)
        curr_x_df.loc[train_idx] = shuffle_col_idx(
            curr_x_df.loc[train_idx], col)
        varsc   = fit_expl_var(curr_x_df, y_df, train_idx, test_idx, stimpar)
        expl_var[col] = full_expl_var - varsc

    return expl_var


#############################################
def compile_dict_fold_stats(dict_list, analyspar):
    
    full_dict = dict()
    all_keys = dict_list[0].keys()
    for key in all_keys:
        fold_vals = np.asarray([sub_dict[key] for sub_dict in dict_list])
        me, de = math_util.get_stats(
            fold_vals, stats=analyspar.stats, error=analyspar.error)
        full_dict[key] = [me, de]
    return full_dict


#############################################
def scale_across_rois(y_df, tr_idx, sessids, stats="mean"):
    """
    """

    y_df_new = copy.deepcopy(y_df)
    sessid_vals = np.unique(sessids)

    tr_idx_set = set(tr_idx)
    for val in sessid_vals:
        all_idx_set = set(np.where(sessids == val)[0])
        curr_tr   = list(all_idx_set.intersection(tr_idx_set))
        curr_test = list(all_idx_set - tr_idx_set)

        scaled_tr, facts = math_util.scale_data(
            y_df.loc[curr_tr].to_numpy(), axis=0, sc_type="stand_rob", 
            nanpol="omit")
        acr_rois_tr   = math_util.mean_med(
            scaled_tr, stats=stats, axis=1, nanpol="omit")
        y_df_new.loc[curr_tr, "roi_data"] = acr_rois_tr

        scaled_test = math_util.scale_data(y_df.loc[curr_test].to_numpy(), 
            axis=0, sc_type="stand_rob", facts=facts, nanpol="omit")
        acr_rois_test = math_util.mean_med(
            scaled_test, stats=stats, axis=1, nanpol="omit")
        y_df_new.loc[curr_test, "roi_data"] = acr_rois_test

    y_df_new = y_df_new[["roi_data"]]

    return y_df_new


#############################################
def run_explained_variance(x_df, y_df, analyspar, stimpar, k=10):
    """
    Consider splitting 80:20 for a test set?

    y: series or dataframe
    """

    x_df = copy.deepcopy(x_df)
    
    y_df_cols = y_df.columns

    if len(y_df_cols) > 1: # multiple ROIs (to scale and average)
        all_rois = True
        if "sessid" in x_df.columns:
            sessids = x_df["sessid"].tolist()
        else:
            sessids = [1] * len(x_df)
        logger.info("Running all ROIs")
    else:
        all_rois = False
        logger.info(f"Running for ROI {y_df_cols[0].replace('roi_data_', '')}")

    kf = model_selection.KFold(k, shuffle=True, random_state=None)

    full, coef_all, coef_uni = [], [], []
    for tr_idx, test_idx in kf.split(x_df):
        if all_rois:
            y_df = scale_across_rois(y_df, tr_idx, sessids, analyspar.stats)
        # one model per category
        full.append(fit_expl_var(x_df, y_df, tr_idx, test_idx, stimpar))
        coef_all.append(fit_expl_var_per_coeff(
            x_df, y_df, tr_idx, test_idx, stimpar))
        coef_uni.append(fit_unique_expl_var_per_coeff(
            x_df, y_df, tr_idx, test_idx, stimpar, full[-1]))

    full = math_util.get_stats(
        np.asarray(full), stats=analyspar.stats, error=analyspar.error).tolist()

    coef_all = compile_dict_fold_stats(coef_all, analyspar)
    coef_uni = compile_dict_fold_stats(coef_uni, analyspar)

    return full, coef_all, coef_uni


#############################################
def run_glm(sessions, analyspar, sesspar, stimpar, glmpar, parallel=False):

    full_df = build_stim_beh_df(sessions, analyspar, sesspar, stimpar, 
                                glmpar.each_roi)
    # run_bayesian_ridge(x_df, y_df, analyspar)
    # run_ridge_cv(x_df, y_df, analyspar)
    # run_ols_summary(x_df, y_df)

    roi_nbrs = [-1]
    roi_cols = [[col for col in full_df.columns if "roi_data_" in col]]
    if glmpar.each_roi:
        for col in roi_cols[0]:
            roi_cols.append([col])
            roi_nbrs.append(int(col.replace("roi_data_", "")))

    x_df = full_df.drop(columns=roi_cols[0])

    if glmpar.test:
        roi_cols = roi_cols[:4]
        roi_nbrs = roi_nbrs[:4]

    # optionally runs in parallel
    if parallel and glmpar.each_roi and len(roi_cols) > 1:
        n_jobs = gen_util.get_n_jobs(len(roi_cols))
        outs = Parallel(n_jobs=n_jobs)(delayed(run_explained_variance)
            (x_df, full_df[cols], analyspar, stimpar, glmpar.k) 
            for cols in roi_cols)
        fulls, coef_alls, coef_unis = zip(*outs)
    else:
        fulls, coef_alls, coef_unis = [], [], []
        for cols in roi_cols:
            out = run_explained_variance(
                x_df, full_df[cols], analyspar, stimpar, glmpar.k)
            fulls.append(out[0])
            coef_alls.append(out[1])
            coef_unis.append(out[2])

    expl_var = {"full"    : fulls,
                "coef_all": gen_util.compile_dict_list(coef_alls),
                "coef_uni": gen_util.compile_dict_list(coef_unis),
                "rois"    : roi_nbrs
                }

    return expl_var


#############################################
def run_glms(sessions, analysis, seed, analyspar, sesspar, stimpar, glmpar, 
             figpar, parallel=False):


    seed = rand_util.seed_all(seed, "cpu", log_seed=False)

    sessstr_pr = sess_str_util.sess_par_str(
        sesspar.sess_n, stimpar.stimtype, sesspar.plane, stimpar.bri_dir, 
        stimpar.bri_size, stimpar.gabk, "print")
    dendstr_pr = sess_str_util.dend_par_str(
        analyspar.dend, sesspar.plane, "roi", "print")

    logger.info("Analysing and plotting explained variance in ROI activity "
        f"({sessstr_pr}{dendstr_pr}).", extra={"spacing": "\n"})

    if glmpar.each_roi: # must do each session separately
        glm_type = "per_ROI_per_sess"
        sess_batches = sessions
        logger.info("Per ROI, each session separately.")
    else:
        glm_type = "across_sess"
        sess_batches = [sessions]
        logger.info(f"Across ROIs, {len(sessions)} sessions together.")

    # optionally runs in parallel, or propagates parallel to next level
    parallel_here = (
        parallel and not(glmpar.each_roi) and (len(sess_batches) != 1)
        )
    parallel_after = True if (parallel and not(parallel_here)) else False

    args_list = [analyspar, sesspar, stimpar, glmpar]
    args_dict = {"parallel": parallel_after} # proactively set next parallel
    all_expl_var = gen_util.parallel_wrap(
        run_glm, sess_batches, args_list, args_dict, parallel=parallel_here
        )
    
    if glmpar.each_roi:
        sessions = sess_batches
    else:
        sessions = sess_batches[0]

    sess_info = sess_gen_util.get_sess_info(
        sessions, analyspar.fluor, remnans=analyspar.remnans
        )

    extrapar = {"analysis": analysis,
                "seed"    : seed,
                "glm_type": glm_type,
                }

    info = {"analyspar"   : analyspar._asdict(),
            "sesspar"     : sesspar._asdict(),
            "stimpar"     : stimpar._asdict(),
            "glmpar"      : glmpar._asdict(),
            "extrapar"    : extrapar,
            "all_expl_var": all_expl_var,
            "sess_info"   : sess_info
            }

    fulldir, savename = glm_plots.plot_glm_expl_var(figpar=figpar, **info)

    file_util.saveinfo(info, savename, fulldir, "json")


        # Sensitivity:
        # - (Average response to A - Average response to not A)/std(data)

    return

